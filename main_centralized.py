# üìÑ main_centralized.py
# ÂÆûÈ™åËÆæÁΩÆ B: Collection (Centralized)
# Ê®°ÊãüÂ∞Ü KG1 Âíå KG2 Êï∞ÊçÆÈõÜ‰∏≠Âà∞‰∏ÄÂè∞Êú∫Âô®ÔºåÊûÑÂª∫Â§ßÂõæËøõË°åËÆ≠ÁªÉ
# Ê≥®ÊÑèÔºöËøôÈúÄË¶ÅÂêàÂπ∂ÈÇªÊé•Áü©ÈòµÔºåÂπ∂Â§ÑÁêÜ ID ÂÅèÁßª

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import config
import data_loader
import precompute
import evaluate
from models.gcn import GCN
from tqdm import tqdm
import os
import utils_logger


def run_centralized_experiment():
    print(f"{'='*60}")
    print("üß™ ÂÆûÈ™å B: Collection (Centralized Training)")
    print(f"   ÁõÆÊ†á: È™åËØÅÊó†ÈöêÁßÅÈôêÂà∂‰∏ãÔºåÂêàÂπ∂ÂõæÁªìÊûÑËÆ≠ÁªÉÁöÑÁêÜËÆ∫‰∏äÈôê")
    print(f"{'='*60}")

    # --- 1. Êï∞ÊçÆÂä†ËΩΩ‰∏éÂêàÂπ∂ ---
    print("\n[1] Êï∞ÊçÆÂêàÂπ∂...")
    ent_1 = data_loader.load_id_map(config.BASE_PATH + "ent_ids_1")
    trip_1 = data_loader.load_triples(config.BASE_PATH + "triples_1")
    num_ent_1 = max(list(ent_1[0].keys())) + 1

    ent_2 = data_loader.load_id_map(config.BASE_PATH + "ent_ids_2")
    trip_2 = data_loader.load_triples(config.BASE_PATH + "triples_2")
    num_ent_2 = max(list(ent_2[0].keys())) + 1

    # „ÄêÂÖ≥ÈîÆ„ÄëID ÂÅèÁßªÔºöKG2 ÁöÑ ID ÈúÄË¶ÅÂä†‰∏ä KG1 ÁöÑÊÄªÊï∞ÔºåÈÅøÂÖçÂÜ≤Á™Å
    offset = num_ent_1
    trip_2_shifted = [(h + offset, r, t + offset) for h, r, t in trip_2]

    triples_all = trip_1 + trip_2_shifted
    total_ent = num_ent_1 + num_ent_2
    print(
        f"   Merged Graph: {num_ent_1} (KG1) + {num_ent_2} (KG2) = {total_ent} Entities")
    print(f"   Merged Edges: {len(triples_all)}")

    # --- 2. È¢ÑËÆ°ÁÆó (Â§ßÂõæÈÇªÊé•Áü©Èòµ & SBERT) ---
    print("\n[2] ÊûÑÂª∫Â§ßÂõæÈÇªÊé•Áü©Èòµ & Âä†ËΩΩ SBERT...")
    adj_all = precompute.build_adjacency_matrix(triples_all, total_ent)

    # Âä†ËΩΩ SBERT (Â§çÁî®ÁºìÂ≠ò)
    sb_1 = precompute.get_bert_embeddings(
        ent_1, {}, "KG1", cache_file="cache/sbert_KG1.pt")
    sb_2 = precompute.get_bert_embeddings(
        ent_2, {}, "KG2", cache_file="cache/sbert_KG2.pt")

    # ÂêàÂπ∂ SBERT Features (‰Ωú‰∏∫ËÆ≠ÁªÉÁõÆÊ†á)
    # ÊûÑÈÄ†‰∏Ä‰∏™Â§ß Tensor [total_ent, 768]
    sbert_target = torch.zeros(total_ent, config.BERT_DIM)

    train_indices = []
    # Â°´ÂÖ• KG1
    for eid, emb in sb_1.items():
        sbert_target[eid] = emb
        train_indices.append(eid)
    # Â°´ÂÖ• KG2 (ËÆ∞ÂæóÂä† offset)
    for eid, emb in sb_2.items():
        sbert_target[eid + offset] = emb
        train_indices.append(eid + offset)

    sbert_target = sbert_target.to(config.DEVICE)
    train_indices = torch.tensor(train_indices).to(config.DEVICE)

    # ÁßªÂä®ÈÇªÊé•Áü©Èòµ (Â¶ÇÊûúÊòØ CUDA)
    if config.DEVICE.type == 'cuda':
        adj_all = adj_all.to(config.DEVICE)

    # --- 3. ÂàùÂßãÂåñÈõÜ‰∏≠ÂºèÊ®°Âûã ---
    print("\n[3] ÂàùÂßãÂåñÈõÜ‰∏≠Âºè GCN...")
    # ËøôÈáåÁõ¥Êé•Áî®‰∏Ä‰∏™Â§ß GCNÔºå‰∏çÈúÄ DecoupledÔºåÂõ†‰∏∫Êï∞ÊçÆÈÉΩÂú®Êú¨Âú∞
    model = GCN(
        num_entities=total_ent,
        feature_dim=config.GCN_DIM,
        hidden_dim=config.GCN_HIDDEN,
        output_dim=config.BERT_DIM,
        dropout=config.GCN_DROPOUT
    ).to(config.DEVICE)

    optimizer = optim.Adam(model.parameters(), lr=config.FL_LR)
    criterion = nn.MarginRankingLoss(margin=config.FL_MARGIN)

    # --- 4. ËÆ≠ÁªÉÂæ™ÁéØ ---
    print("\n[4] ÂºÄÂßãËÆ≠ÁªÉ...")
    epochs = 200  # ÈõÜ‰∏≠ÂºèÈÄöÂ∏∏Êî∂ÊïõËæÉÂø´ÔºåÊàñËÄÖËÆæ‰∏∫‰∏éËÅîÈÇ¶ÊÄªËΩÆÊ¨°Áõ∏ÂΩì

    model.train()
    for epoch in tqdm(range(epochs), desc="Training"):
        optimizer.zero_grad()
        output = model(adj_all)

        # ÈîöÁÇπÂØπÈΩê Loss
        out_batch = output[train_indices]
        target_batch = sbert_target[train_indices]

        pos_sim = F.cosine_similarity(out_batch, target_batch)

        # ÁÆÄÂçïË¥üÈááÊ†∑
        # ÂÆûÈôÖ‰ª£Á†Å‰∏≠ÂèØ‰ª•‰ΩøÁî® fl_core ÈáåÊõ¥Â§çÊùÇÁöÑ hard miningÔºåËøôÈáå‰∏∫‰∫ÜÊºîÁ§∫‰øùÊåÅÁÆÄÊ¥Å
        # ‰ΩøÁî®ÈöèÊú∫Ë¥üÈááÊ†∑Ê®°Êãü
        perm = torch.randperm(len(target_batch)).to(config.DEVICE)
        neg_target = target_batch[perm]
        neg_sim = F.cosine_similarity(out_batch, neg_target)

        loss = criterion(pos_sim, neg_sim, torch.ones_like(pos_sim))
        loss.backward()
        optimizer.step()

    # --- 5. ËØÑ‰º∞ ---
    print("\n[5] ÊúÄÁªàËØÑ‰º∞...")
    model.eval()
    with torch.no_grad():
        embeddings_all = model(adj_all).detach().cpu()

    # ÊãÜÂàÜ Embedding ÂõûÂéª
    # KG1: 0 ~ num_ent_1
    emb_1 = {i: embeddings_all[i] for i in range(num_ent_1)}

    # KG2: offset ~ total (Ê≥®ÊÑèÔºöKey Ë¶ÅÂáèÂéª offset ÂèòÂõûÂéüÂßã IDÔºå‰ª•‰æøËØÑ‰º∞Âô®ËØÜÂà´)
    emb_2 = {i: embeddings_all[i + offset] for i in range(num_ent_2)}

    test_pairs = data_loader.load_alignment_pairs(
        config.BASE_PATH + "ref_pairs")

    # Â§çÁî® evaluate Ê®°Âùó (Ê≠§Êó∂Ê®°ÂûãÂ∑≤ÂåÖÂê´ÁªìÊûÑ‰ø°ÊÅØÔºåËÆæ Alpha=1.0 Á∫ØÁªìÊûÑÔºåÊàñËÄÖ 0.42 ËûçÂêà)
    print("   [Mode] Evaluation with Fusion (Alpha=0.42)")
    hits, mrr = evaluate.evaluate_alignment(
        test_pairs, emb_1, emb_2,
        nn.Identity(), nn.Identity(),  # Ê®°ÂûãÂ∑≤Êé®ÁêÜÂÆåÊØïÔºå‰º†ÂÖ• Identity
        config.EVAL_K_VALUES,
        sbert_1=sb_1, sbert_2=sb_2,
        alpha=config.EVAL_FUSION_ALPHA
    )

    # ---> Êñ∞Â¢ûËÆ∞ÂΩï‰ª£Á†Å
    utils_logger.log_experiment_result(
        exp_name="Collection (Centralized)",
        dataset=config.CURRENT_DATASET_NAME,
        metrics={"hits1": hits[1], "hits10": hits[10], "mrr": mrr},
        params={"epochs": 200}
    )


if __name__ == "__main__":
    run_centralized_experiment()
