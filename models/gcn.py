# ğŸ“„ models/gcn.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class GCNLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim, bias=False)

    def forward(self, x, adj):
        # 1. çº¿æ€§å˜æ¢ (GPU åŠ é€Ÿ)
        x = self.linear(x)

        # 2. é‚»å±…èšåˆ (CPU/GPU æ··åˆè®¡ç®—å…¼å®¹)
        # å¦‚æœ x åœ¨ MPS (GPU) ä¸Šï¼Œä½† adj åœ¨ CPU ä¸Šï¼Œåˆ™å°† x ä¸´æ—¶è½¬åˆ° CPU è¿›è¡Œç¨€ç–ä¹˜æ³•
        if x.device.type == 'mps' and adj.device.type == 'cpu':
            out = torch.sparse.mm(adj, x.cpu()).to(x.device)
        else:
            out = torch.sparse.mm(adj, x)

        return out


class GCN(nn.Module):
    def __init__(self, num_entities, feature_dim, hidden_dim, output_dim, dropout=0.3):
        super().__init__()

        print(
            f"    [Model Init] GCN: {feature_dim} -> {hidden_dim} -> {output_dim} (Dropout: {dropout})")

        # åˆå§‹ç»“æ„ç‰¹å¾ (Node Embeddings)
        self.initial_features = nn.Parameter(
            torch.randn(num_entities, feature_dim))
        nn.init.xavier_uniform_(self.initial_features)

        self.gc1 = GCNLayer(feature_dim, hidden_dim)
        self.gc2 = GCNLayer(hidden_dim, output_dim)

        self.dropout = dropout

    def forward(self, adj):
        x = self.initial_features

        # Layer 1
        x = self.gc1(x, adj)
        x = F.relu(x)
        x = F.dropout(x, self.dropout, training=self.training)

        # Layer 2
        x = self.gc2(x, adj)

        return x
