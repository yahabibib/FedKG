# ğŸ“„ models/gcn.py
import torch
import torch.nn as nn
import torch.nn.functional as F


# ğŸ“„ models/gcn.py (ç‰‡æ®µï¼šä»…æ›¿æ¢ Layer ç±»)

class RelationGCNLayer(nn.Module):
    def __init__(self, input_dim, output_dim, activation=F.relu):
        super().__init__()
        self.linear = nn.Linear(input_dim, output_dim, bias=False)
        self.activation = activation

        # âš¡ï¸ [ä¼˜åŒ–] ç»´åº¦å‡åŠï¼šåŸæ¥æ˜¯ input*2 -> 1ï¼Œç°åœ¨ input -> 1
        self.attn_kernel = nn.Linear(input_dim, 1, bias=False)
        nn.init.xavier_uniform_(self.attn_kernel.weight)

    def forward(self, x, edge_index, edge_type, rel_emb):
        x_trans = self.linear(x)
        src_idx, tgt_idx = edge_index

        rel_emb = rel_emb.to(x.device)

        # å–å‡ºç‰¹å¾
        h_src = x[src_idx]
        h_rel = rel_emb[edge_type]

        # âš¡ï¸ [ä¼˜åŒ–] è½»é‡åŒ–è®¡ç®—: ä½¿ç”¨åŠ æ³•ä»£æ›¿æ‹¼æ¥ (Save Memory!)
        # h_src: [E, D], h_rel: [E, D] -> sum_feat: [E, D]
        # ç›¸æ¯”ä¹‹å‰çš„ cat ([E, 2D]), èŠ‚çœäº†ä¸€åŠå†…å­˜
        sum_feat = h_src + h_rel

        # è®¡ç®— Attention
        attn_weights = torch.sigmoid(self.attn_kernel(sum_feat))

        # æ¶ˆæ¯ä¼ é€’
        msg = x_trans[src_idx] * attn_weights

        # èšåˆ
        out = torch.zeros(x.shape[0], x_trans.shape[1], device=x.device)
        out.index_add_(0, tgt_idx, msg)

        if self.activation:
            out = self.activation(out)
        return out


class RelationGCN(nn.Module):
    def __init__(self, num_entities, num_relations, feature_dim, hidden_dim, output_dim, dropout=0.3):
        super().__init__()
        print(
            f"    [Model Init] Relation-Aware GCN: Utilizing {num_relations} relations.")

        # å®ä½“åˆå§‹ç‰¹å¾
        self.initial_features = nn.Parameter(
            torch.randn(num_entities, feature_dim))
        nn.init.xavier_uniform_(self.initial_features)

        # å…³ç³»åµŒå…¥ (å¯å­¦ä¹ ï¼Œä½†ç”¨ SBERT åˆå§‹åŒ–)
        # æˆ‘ä»¬ä¼šåœ¨ forward é‡Œæ¥æ”¶ SBERT åˆå§‹å€¼ï¼Œæˆ–è€…åœ¨è¿™é‡Œå®šä¹‰ Parameter
        # ä¸ºäº†çµæ´»æ€§ï¼Œæˆ‘ä»¬å®šä¹‰ä¸º Parameterï¼Œåˆå§‹åŒ–æ—¶åŠ è½½ SBERT
        self.relation_embeddings = nn.Parameter(
            torch.randn(num_relations, feature_dim))

        # å±‚å®šä¹‰
        self.gc1 = RelationGCNLayer(feature_dim, hidden_dim)
        self.gc2 = RelationGCNLayer(
            hidden_dim, output_dim, activation=None)  # æœ€åä¸€å±‚é€šå¸¸ä¸åŠ æ¿€æ´»

        self.dropout = dropout

    def init_relation_embeddings(self, sbert_rel_emb):
        """ ç”¨ SBERT åˆå§‹åŒ–å…³ç³»åµŒå…¥ """
        with torch.no_grad():
            for rid, emb in sbert_rel_emb.items():
                if rid < self.relation_embeddings.shape[0]:
                    # å‡è®¾ SBERT æ˜¯ 768ï¼Œfeature_dim æ˜¯ 300ï¼Œéœ€è¦æŠ•å½±æˆ–æˆªæ–­
                    # å¦‚æœ feature_dim != 768, å»ºè®®åŠ ä¸ªçº¿æ€§å±‚æŠ•å½±ï¼Œè¿™é‡Œç®€å•èµ·è§å‡è®¾ç»´åº¦åŒ¹é…
                    # æˆ–è€…æˆ‘ä»¬åœ¨å¤–éƒ¨åšå¥½æŠ•å½±ã€‚è¿™é‡Œå…ˆåªå¤åˆ¶èƒ½å¤åˆ¶çš„éƒ¨åˆ†ã€‚
                    dim = min(self.relation_embeddings.shape[1], emb.shape[0])
                    self.relation_embeddings.data[rid, :dim] = emb[:dim]

    def forward(self, edge_index, edge_type):
        x = self.initial_features

        # Layer 1
        x = self.gc1(x, edge_index, edge_type, self.relation_embeddings)
        x = F.dropout(x, self.dropout, training=self.training)

        # Layer 2
        x = self.gc2(x, edge_index, edge_type, self.relation_embeddings)

        return x
