# ğŸ“„ models/gcn.py
# ã€RREA å®‰å…¨ç‰ˆã€‘ç§»é™¤è¾“å…¥å¼ºåˆ¶å½’ä¸€åŒ–ï¼Œä¿ç•™å…³ç³»å½’ä¸€åŒ–ï¼Œå¢å¼ºæ•°å€¼ç¨³å®šæ€§

import torch
import torch.nn as nn
import torch.nn.functional as F


class ReflectionLayer(nn.Module):
    def __init__(self, in_channels, output_dim, activation=F.relu):
        super().__init__()
        self.in_channels = in_channels
        self.activation = activation

        # Shape Builder å˜æ¢çŸ©é˜µ
        self.W = nn.Linear(in_channels, output_dim, bias=False)
        nn.init.xavier_uniform_(self.W.weight)

        # å†…ç½® LayerNorm (è¿™æ˜¯é˜²çˆ†çš„å…³é”®ï¼Œå¿…é¡»ä¿ç•™)
        self.norm = nn.LayerNorm(output_dim)

    def forward(self, x, edge_index, edge_type, rel_emb):
        """
        x: [N, D]
        rel_emb: [TotalRels, D]
        """
        src_idx, tgt_idx = edge_index

        # 1. å‡†å¤‡å…³ç³»å‘é‡
        rel_emb = rel_emb.to(x.device)

        # âš ï¸ å¿…é¡»å½’ä¸€åŒ–å…³ç³»å‘é‡ (||r||=1)ï¼Œå¦åˆ™åå°„å…¬å¼ä¸æˆç«‹
        # å¢åŠ  eps é˜²æ­¢é™¤é›¶å¼‚å¸¸
        rel_emb = F.normalize(rel_emb, p=2, dim=1, eps=1e-6)

        # 2. å‡†å¤‡èŠ‚ç‚¹ç‰¹å¾
        # âŒ ç§»é™¤å¯¹ x çš„å¼ºåˆ¶å½’ä¸€åŒ–ï¼Œé¿å…æ¢¯åº¦é—®é¢˜ï¼Œè®© LayerNorm å»å¤„ç†å°ºåº¦
        h_src = x[src_idx]
        h_rel = rel_emb[edge_type]

        # 3. å…³ç³»åå°„å˜æ¢ (Relational Reflection)
        # å…¬å¼: h' = h - 2 * (h . r) * r

        # è®¡ç®—ç‚¹ç§¯
        dot_prod = torch.sum(h_src * h_rel, dim=1, keepdim=True)

        # æ‰§è¡Œåå°„
        h_reflected = h_src - 2 * dot_prod * h_rel

        # 4. èšåˆ (Mean Aggregation)
        out = torch.zeros(x.shape[0], h_reflected.shape[1], device=x.device)
        out.index_add_(0, tgt_idx, h_reflected)

        # åº¦å½’ä¸€åŒ–
        ones = torch.ones(tgt_idx.size(0), 1, device=x.device)
        deg = torch.zeros(x.shape[0], 1, device=x.device)
        deg.index_add_(0, tgt_idx, ones)
        out = out / deg.clamp(min=1.0)

        # 5. çº¿æ€§å˜æ¢ (Shape Building)
        out = self.W(out)

        # 6. æ®‹å·®è¿æ¥ (Residual)
        # åªæœ‰ç»´åº¦åŒ¹é…æ—¶æ‰åŠ æ®‹å·®
        if out.shape == x.shape:
            out = out + x

        # 7. è¾“å‡ºç¨³å‹ (LayerNorm)
        out = self.norm(out)

        if self.activation:
            out = self.activation(out)

        return out


class RelationGCN(nn.Module):
    def __init__(self, num_entities, num_relations, feature_dim, hidden_dim, output_dim, dropout=0.3):
        super().__init__()

        self.num_base_relations = num_relations
        total_rels = 2 * num_relations + 1

        print(
            f"    [Model Init] RREA (Safe): {num_entities} Ents, {total_rels} Rels")

        self.initial_features = nn.Parameter(
            torch.randn(num_entities, feature_dim))
        nn.init.xavier_uniform_(self.initial_features)

        self.relation_embeddings = nn.Parameter(
            torch.randn(total_rels, feature_dim))
        nn.init.xavier_uniform_(self.relation_embeddings)

        # å®šä¹‰ä¸¤å±‚
        self.gc1 = ReflectionLayer(feature_dim, hidden_dim)
        self.gc2 = ReflectionLayer(hidden_dim, output_dim, activation=None)

        self.dropout = dropout

    def init_relation_embeddings(self, sbert_rel_emb):
        pass

    def forward(self, edge_index, edge_type):
        x = self.initial_features

        x = self.gc1(x, edge_index, edge_type, self.relation_embeddings)
        x = F.dropout(x, self.dropout, training=self.training)
        x = self.gc2(x, edge_index, edge_type, self.relation_embeddings)

        return x
