# ğŸ“„ models/decoupled.py
import torch
import torch.nn as nn
import torch.nn.functional as F
from .gcn import RelationGCNLayer  # <--- å‡çº§å¯¼å…¥ RelationGCNLayer


class DecoupledModel(nn.Module):
    """
    ã€å…³ç³»æ„ŸçŸ¥è§£è€¦æ¨¡å‹ã€‘
    - struct_encoder (RelationGCN): ç§æœ‰å±‚ï¼Œæå–å…³ç³»æ„ŸçŸ¥çš„ç»“æ„ç‰¹å¾
    - semantic_projector (MLP): å…¬å…±å±‚ï¼Œæ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´
    """

    def __init__(self, num_entities, num_relations, feature_dim, hidden_dim, output_dim, dropout=0.3):
        super().__init__()

        # æ³¨æ„ï¼šnum_relations æ˜¯åŸå§‹å…³ç³»æ•°ï¼Œå†…éƒ¨å±‚ä¼šå¤„ç†åå‘å’Œè‡ªç¯ï¼Œ
        # ä½†æˆ‘ä»¬éœ€è¦åœ¨è¿™é‡Œç»´æŠ¤ Relation Embeddingsï¼Œå› ä¸ºå®ƒæ˜¯ç»“æ„ç¼–ç å™¨çš„ä¸€éƒ¨åˆ†
        total_rels = 2 * num_relations + 1

        print(
            f"    [Model Init] Decoupled (Relation-Aware): {num_entities} Ents, {total_rels} Rels")

        # --- 1. ç§æœ‰ç»“æ„ç¼–ç å™¨ (Private) ---
        # åˆå§‹èŠ‚ç‚¹ç‰¹å¾
        self.initial_features = nn.Parameter(
            torch.randn(num_entities, feature_dim))
        nn.init.xavier_uniform_(self.initial_features)

        # åˆå§‹å…³ç³»ç‰¹å¾ (ç§æœ‰å‚æ•°)
        self.relation_embeddings = nn.Parameter(
            torch.randn(total_rels, feature_dim))
        nn.init.xavier_uniform_(self.relation_embeddings)

        # ä½¿ç”¨ RelationGCNLayer æ›¿ä»£åŸæ¥çš„ GCNLayer
        self.struct_encoder = nn.ModuleList([
            RelationGCNLayer(feature_dim, hidden_dim),
            # å¯ä»¥åŠ æ›´å¤šå±‚ï¼Œè¿™é‡Œä¿æŒåŒå±‚ç»“æ„
            # ç¬¬äºŒå±‚è¾“å…¥ hidden, è¾“å‡º hidden (å› ä¸ºæœ€åè¿˜è¦è¿‡ MLP)
            # æˆ–è€…åƒ GCN é‚£æ ·ç¬¬äºŒå±‚ç›´æ¥è¾“å‡º output_dim?
            # è¿™é‡Œçš„ Decoupled æ¶æ„é€šå¸¸æ˜¯ GCN è´Ÿè´£æå–ç‰¹å¾ï¼ŒMLP è´Ÿè´£å¯¹é½
            # æ‰€ä»¥æˆ‘ä»¬è®© GCN è¾“å‡º hidden_dim
        ])

        # ä¸ºäº†åŠ æ·±ç½‘ç»œï¼Œå¯ä»¥åŠ ä¸€ä¸ªå±‚é—´æ¿€æ´»
        self.activation = nn.ReLU()

        # --- 2. å…¬å…±è¯­ä¹‰æ˜ å°„å™¨ (Shared) ---
        # MLP: Hidden -> Output (SBERT Dim)
        self.semantic_projector = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, output_dim)
        )

        self.dropout = dropout

    def init_relation_embeddings(self, sbert_rel_emb):
        """ ç”¨ SBERT åˆå§‹åŒ–å…³ç³»åµŒå…¥ """
        with torch.no_grad():
            count = 0
            for rid, emb in sbert_rel_emb.items():
                if rid < self.relation_embeddings.shape[0]:
                    dim = min(self.relation_embeddings.shape[1], emb.shape[0])
                    self.relation_embeddings.data[rid, :dim] = emb[:dim]
                    count += 1
            print(f"    [Decoupled] Initialized {count} relation embeddings.")

    def forward(self, edge_index, edge_type):
        x = self.initial_features

        # 1. ç»è¿‡ç§æœ‰ RelationGCN ç¼–ç 
        # æ³¨æ„ï¼šç°åœ¨éœ€è¦ä¼ å…¥ edge_index, edge_type, relation_embeddings
        for i, layer in enumerate(self.struct_encoder):
            x = layer(x, edge_index, edge_type, self.relation_embeddings)
            if i < len(self.struct_encoder) - 1:  # å¦‚æœæœ‰å¤šå±‚
                x = self.activation(x)
                x = F.dropout(x, self.dropout, training=self.training)

        # 2. ç»è¿‡å…¬å…± MLP æ˜ å°„
        x = self.semantic_projector(x)

        return x
