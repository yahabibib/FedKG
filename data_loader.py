# ğŸ“„ data_loader.py
# è´Ÿè´£åŠ è½½æ‰€æœ‰æ–‡ä»¶ (triples, ids, atts, pairs)
# ã€ç»ˆæä¿®å¤ç‰ˆã€‘å¢å¼ºäº† ID æ–‡ä»¶çš„è§£æèƒ½åŠ›ï¼Œä¿®å¤åˆ†éš”ç¬¦é—®é¢˜

import pandas as pd
from collections import defaultdict
import re
import pickle
import os


def load_id_map(file_path):
    """åŠ è½½ ent_ids æˆ– rel_ids æ–‡ä»¶"""
    id_to_uri = {}
    uri_to_id = {}
    filename = file_path.split('/')[-1]
    print(f"  [Data Loader] Loading IDs from: {filename}")

    count = 0
    try:
        # ä½¿ç”¨ utf-8-sig ä»¥å¤„ç†å¯èƒ½çš„ BOM å¤´
        with open(file_path, 'r', encoding='utf-8-sig') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                # 1. å°è¯• Tab åˆ†éš”
                parts = line.split('\t')
                # 2. å¦‚æœ Tab åˆ†éš”å¤±è´¥ (åªæœ‰1åˆ—)ï¼Œå°è¯•ç©ºæ ¼åˆ†éš”
                if len(parts) < 2:
                    parts = line.split()

                if len(parts) >= 2:
                    # é€šå¸¸ç¬¬ä¸€åˆ—æ˜¯ IDï¼Œç¬¬äºŒåˆ—æ˜¯ URI
                    if parts[0].isdigit():
                        ent_id = int(parts[0])
                        uri = parts[1].strip()

                        id_to_uri[ent_id] = uri
                        uri_to_id[uri] = ent_id
                        count += 1
                    else:
                        # å¯èƒ½æ˜¯åè¿‡æ¥çš„? (æå°‘è§ï¼Œä½†é˜²ä¸€æ‰‹)
                        pass
    except Exception as e:
        print(f"  [Error] Failed to load ID map {filename}: {e}")

    print(f"    > Loaded {count} IDs. Example: {list(uri_to_id.items())[:2]}")
    return id_to_uri, uri_to_id


def load_triples(file_path):
    """åŠ è½½ triples æ–‡ä»¶ (head_id, rel_id, tail_id)"""
    triples = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            # åŒæ ·å°è¯•ç©ºæ ¼åˆ†éš”
            if len(parts) < 3:
                parts = line.split()

            if len(parts) >= 3:
                try:
                    h, r, t = int(parts[0]), int(parts[1]), int(parts[2])
                    triples.append((h, r, t))
                except ValueError:
                    continue
    return triples


def load_alignment_pairs(file_path):
    """åŠ è½½ ref_pairs (æµ‹è¯•é›†)"""
    pairs = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            parts = line.strip().split('\t')
            if len(parts) < 2:
                parts = line.split()

            if len(parts) >= 2:
                try:
                    e1, e2 = int(parts[0]), int(parts[1])
                    pairs.append((e1, e2))
                except ValueError:
                    continue
    return pairs


def load_attribute_triples(file_path, ent_map):
    """
    ã€å…¨èƒ½åŠ è½½å™¨ã€‘æ”¯æŒå¤šç§æ ¼å¼ï¼Œå¹¶è‡ªåŠ¨å°è¯•åŒ¹é… URI
    """
    filename = file_path.split('/')[-1]
    print(f"  [Data Loader] Reading attributes from: {filename}")

    uri_to_id = ent_map[1]

    # NT æ ¼å¼æ­£åˆ™: <Subject> <Pred> "Obj" .
    # é’ˆå¯¹æ‚¨æä¾›çš„æ•°æ®: <http://...> <...> "..."@en .
    nt_pattern = re.compile(r'^<([^>]+)>\s+<([^>]+)>\s+(.+?)\s*[\."]*$')

    entity_descriptions = defaultdict(list)
    skipped_count = 0
    valid_count = 0

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                # --- 1. å°è¯• Tab åˆ†éš” (ID æ ¼å¼) ---
                parts = line.split('\t')
                if len(parts) >= 3 and parts[0].isdigit():
                    ent_id = int(parts[0])
                    attr_name = parts[1].split('/')[-1]
                    value = _clean_value(parts[2])
                    entity_descriptions[ent_id].append(
                        f"{attr_name} is {value}")
                    valid_count += 1
                    continue

                # --- 2. å°è¯• N-Triples æ­£åˆ™ ---
                match = nt_pattern.match(line)
                if match:
                    # http://dbpedia.org/resource/Liverpool_F.C.
                    raw_subj = match.group(1)
                    raw_pred = match.group(2)
                    raw_obj = match.group(3)

                    # ã€æ ¸å¿ƒé€»è¾‘ã€‘æ™ºèƒ½æŸ¥æ‰¾ ID
                    # è¿™é‡Œçš„ raw_subj åº”è¯¥èƒ½ç›´æ¥åŒ¹é…åˆ° ent_ids é‡Œçš„ URI
                    ent_id = _find_id_flexible(raw_subj, uri_to_id)

                    if ent_id is None:
                        skipped_count += 1
                        # è°ƒè¯•ï¼šåªæ‰“å°å‰3ä¸ªå¤±è´¥çš„ï¼Œé¿å…åˆ·å±
                        if skipped_count <= 3:
                            print(f"    [Debug Fail] Unmapped URI: {raw_subj}")
                        continue

                    attr_name = raw_pred.split('/')[-1]
                    value = _clean_value(raw_obj)

                    entity_descriptions[ent_id].append(
                        f"{attr_name} is {value}")
                    valid_count += 1
                else:
                    skipped_count += 1

    except Exception as e:
        print(f"  [Error] reading {file_path}: {e}")
        return {}

    if skipped_count > 0:
        print(
            f"  [Data Loader] Skipped {skipped_count} lines (unmapped). Loaded {valid_count} descriptions.")

    final_descriptions = {
        ent_id: "; ".join(sentences)
        for ent_id, sentences in entity_descriptions.items()
    }
    return final_descriptions


def load_pickle_descriptions(file_path, ent_map):
    """
    ã€æ–°åŠŸèƒ½ã€‘ç›´æ¥åŠ è½½ .pkl æ ¼å¼çš„é«˜è´¨é‡æè¿°æ–‡ä»¶
    file_path: description.pkl çš„è·¯å¾„
    ent_map: (id_to_uri, uri_to_id) ç”¨äºéªŒè¯å’Œå¯¹é½
    """
    filename = file_path.split('/')[-1]
    print(f"  [Data Loader] Loading descriptions from Pickle: {filename}")

    if not os.path.exists(file_path):
        print(f"  [Error] Pickle file not found: {file_path}")
        return {}

    try:
        with open(file_path, 'rb') as f:
            data = pickle.load(f)

        uri_to_id = ent_map[1]
        id_to_uri = ent_map[0]

        final_descriptions = {}
        mapped_count = 0

        # éå† Pickle ä¸­çš„æ•°æ®
        for key, desc in data.items():
            ent_id = None

            # æƒ…å†µ A: Key æ˜¯ URI å­—ç¬¦ä¸² (æœ€å¸¸è§)
            if isinstance(key, str):
                # å°è¯•ç›´æ¥åŒ¹é…
                if key in uri_to_id:
                    ent_id = uri_to_id[key]
                # å°è¯•å»æ‰å°–æ‹¬å·åŒ¹é… <http...> -> http...
                elif key.strip('<>') in uri_to_id:
                    ent_id = uri_to_id[key.strip('<>')]

            # æƒ…å†µ B: Key å·²ç»æ˜¯æ•°å­— ID (ç›´æ¥ç”¨)
            elif isinstance(key, int):
                if key in id_to_uri:
                    ent_id = key

            # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„ IDï¼Œä¸”æè¿°æœ‰æ•ˆ
            if ent_id is not None and desc:
                # æ¸…æ´—ä¸€ä¸‹æè¿° (å»æ‰å¤šä½™ç©ºç™½)
                clean_desc = str(desc).strip()
                # åŠ ä¸Šåå­—å‰ç¼€ï¼Œå¢å¼ºè¯­ä¹‰ (å¦‚ "Linuxå†…æ ¸. Linuxå†…æ ¸æ˜¯...")
                name = id_to_uri[ent_id].split('/')[-1].replace('_', ' ')
                final_descriptions[ent_id] = f"{name}. {clean_desc}"[
                    :500]  # æˆªæ–­é˜²æ­¢è¿‡é•¿
                mapped_count += 1

        print(
            f"  [Pickle Loader] Successfully mapped {mapped_count} descriptions.")
        return final_descriptions

    except Exception as e:
        print(f"  [Error] Failed to load pickle: {e}")
        return {}


def _find_id_flexible(uri, mapping):
    """å°è¯•å¤šç§å˜ä½“æ¥æŸ¥æ‰¾ ID"""
    # 1. ç²¾ç¡®åŒ¹é… (æœ€å¯èƒ½çš„æƒ…å†µ)
    if uri in mapping:
        return mapping[uri]

    # 2. å°è¯•åŠ ä¸Šå°–æ‹¬å· <uri>
    if f"<{uri}>" in mapping:
        return mapping[f"<{uri}>"]

    # 3. å°è¯•åªå–æœ€åä¸€éƒ¨åˆ† (Short Name)
    short_name = uri.split('/')[-1]
    if short_name in mapping:
        return mapping[short_name]

    # 4. å°è¯•å»æ‰ç»“å°¾å¯èƒ½çš„ > (å®¹é”™)
    if uri.strip('<>') in mapping:
        return mapping[uri.strip('<>')]

    return None


def _clean_value(val_str):
    """æ¸…æ´—å±æ€§å€¼ï¼šå»æ‰ç±»å‹åç¼€ã€è¯­è¨€æ ‡ç­¾ã€å¼•å·"""
    # ä¾‹å­: "at FSV Frankfurt..."@en
    # ä¾‹å­: "60"^^<http://...>

    # 1. å»æ‰ç±»å‹åç¼€ ^^<...>
    val = val_str.split('^^')[0]

    # 2. å»æ‰è¯­è¨€æ ‡ç­¾ @en (ä»å³è¾¹æ‰¾æœ€åä¸€ä¸ª@)
    # æ³¨æ„ï¼šå†…å®¹é‡Œå¯èƒ½æœ‰@ï¼Œæ‰€ä»¥è¦å°å¿ƒï¼Œé€šå¸¸è¯­è¨€æ ‡ç­¾åœ¨å¼•å·å¤–
    if val.endswith('@en') or val.endswith('@zh') or val.endswith('@fr') or val.endswith('@ja'):
        val = val.rsplit('@', 1)[0]

    # 3. å»æ‰é¦–å°¾å¼•å·
    val = val.strip('"')
    return val
