# ğŸ“„ step1_build_noisy_mini.py
# æ„å»ºå«å™ªéŸ³çš„è¿·ä½ æ•°æ®é›†ï¼š10 å¯¹é½ + 40 å™ªéŸ³/æ¯ä¾§
# ä¿æŒé—­ç¯å­å›¾ç»“æ„ï¼ŒID é‡æ˜ å°„

import os
import random
import pickle
import config
import data_loader
import logging

logging.basicConfig(level=logging.INFO, format='%(message)s')


def ensure_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)


def load_pickle_robust(path, ent_map):
    if not os.path.exists(path):
        return {}
    try:
        with open(path, 'rb') as f:
            data = pickle.load(f)
    except:
        return {}

    uri2id = {v: k for k, v in ent_map.items()}
    clean_data = {}
    for k, v in data.items():
        if isinstance(k, int) and k in ent_map:
            clean_data[k] = v
        elif isinstance(k, str) and k.strip('<>') in uri2id:
            clean_data[uri2id[k.strip('<>')]] = v
    return clean_data


def run():
    print(f"{'='*60}")
    print("ğŸ—ï¸ æ­¥éª¤ä¸€ï¼šæ„å»ºå«å™ªéŸ³çš„é—­ç¯ Mini-DBP15K (Noisy Subgraph)")
    print(f"   ç­–ç•¥: 10 Core Pairs + 40 Noise Entities per KG")
    print(f"{'='*60}")

    save_path = "data/demo_mini/zh_en"
    ensure_dir(save_path)

    # 1. åŠ è½½å…¨é‡æ•°æ®
    print("ğŸ“š åŠ è½½å…¨é‡ DBP15K æ•°æ®...")
    ent_1, _ = data_loader.load_id_map(config.BASE_PATH + "ent_ids_1")
    ent_2, _ = data_loader.load_id_map(config.BASE_PATH + "ent_ids_2")
    rel_1, _ = data_loader.load_id_map(config.BASE_PATH + "rel_ids_1")
    rel_2, _ = data_loader.load_id_map(config.BASE_PATH + "rel_ids_2")
    trip_1 = data_loader.load_triples(config.BASE_PATH + "triples_1")
    trip_2 = data_loader.load_triples(config.BASE_PATH + "triples_2")
    ref_pairs = data_loader.load_alignment_pairs(
        config.BASE_PATH + "ref_pairs")

    print("ğŸ“ åŠ è½½æè¿°æ–‡æœ¬...")
    attr_1 = load_pickle_robust(config.BASE_PATH + "description1.pkl", ent_1)
    attr_2 = load_pickle_robust(config.BASE_PATH + "description2.pkl", ent_2)

    # 2. é‡‡æ ·é€»è¾‘
    # ç­›é€‰"å¯Œå®ä½“"
    adj_1 = {h for h, r, t in trip_1}
    adj_2 = {h for h, r, t in trip_2}
    rich_pairs = [p for p in ref_pairs if p[0] in attr_1 and p[1]
                  in attr_2 and p[0] in adj_1 and p[1] in adj_2]

    # A. æ ¸å¿ƒå¯¹é½ (10å¯¹)
    core_pairs = random.sample(rich_pairs, 10)
    core_ids_1 = set([p[0] for p in core_pairs])
    core_ids_2 = set([p[1] for p in core_pairs])

    # B. å™ªéŸ³å®ä½“ (å„40ä¸ªï¼Œäº’ä¸é‡å ï¼Œä¸”ä¸åŒ…å«æ ¸å¿ƒ)
    # ä»å‰©ä½™å®ä½“ä¸­é€‰
    remain_1 = [
        e for e in ent_1 if e not in core_ids_1 and e in attr_1 and e in adj_1]
    remain_2 = [
        e for e in ent_2 if e not in core_ids_2 and e in attr_2 and e in adj_2]

    noise_ids_1 = set(random.sample(remain_1, 40))
    noise_ids_2 = set(random.sample(remain_2, 40))

    target_ids_1 = core_ids_1.union(noise_ids_1)
    target_ids_2 = core_ids_2.union(noise_ids_2)

    print(f"   âœ… KG1: 10 Core + 40 Noise = {len(target_ids_1)}")
    print(f"   âœ… KG2: 10 Core + 40 Noise = {len(target_ids_2)}")

    # 3. å­å›¾æå– (åŒ…å«å°¾å®ä½“æè¿°çš„é—­ç¯æ„å»º)
    def process_subgraph(targets, full_trips, full_ents, full_rels, full_attr, suffix):
        print(f"   ğŸ”¨ å¤„ç† KG{suffix} å­å›¾...")
        mini_triples = []

        # æ”¶é›†æ‰€æœ‰æ¶‰åŠçš„å®ä½“ (Head + Tail)
        used_ents = set(targets)  # é¦–å…ˆåŒ…å«æ‰€æœ‰ç›®æ ‡å¤´å®ä½“
        used_rels = set()

        # æå–ä»¥ targets ä¸ºå¤´çš„ä¸‰å…ƒç»„
        for h, r, t in full_trips:
            if h in targets:
                mini_triples.append((h, r, t))
                used_ents.add(t)  # å°¾å®ä½“å¿…é¡»åŠ å…¥ï¼Œå¦åˆ™å›¾æ˜¯æ–­çš„
                used_rels.add(r)

        # ID é‡æ˜ å°„
        sorted_ents = sorted(list(used_ents))
        sorted_rels = sorted(list(used_rels))
        old2new_ent = {old: new for new, old in enumerate(sorted_ents)}
        old2new_rel = {old: new for new, old in enumerate(sorted_rels)}

        # ä¿å­˜æ–‡ä»¶
        with open(os.path.join(save_path, f"ent_ids_{suffix}"), 'w', encoding='utf-8') as f:
            for old_id in sorted_ents:
                f.write(f"{old2new_ent[old_id]}\t{full_ents[old_id]}\n")

        with open(os.path.join(save_path, f"rel_ids_{suffix}"), 'w', encoding='utf-8') as f:
            for old_id in sorted_rels:
                f.write(f"{old2new_rel[old_id]}\t{full_rels[old_id]}\n")

        with open(os.path.join(save_path, f"triples_{suffix}"), 'w', encoding='utf-8') as f:
            for h, r, t in mini_triples:
                f.write(
                    f"{old2new_ent[h]}\t{old2new_rel[r]}\t{old2new_ent[t]}\n")

        # ä¿å­˜æè¿° (æ‰€æœ‰æ¶‰åŠçš„å®ä½“ï¼ŒåŒ…æ‹¬ä½œä¸ºå°¾å®ä½“çš„å¶å­èŠ‚ç‚¹)
        mini_desc = {}
        count = 0
        for old_id in sorted_ents:
            if old_id in full_attr:
                mini_desc[old2new_ent[old_id]] = full_attr[old_id]
                count += 1

        with open(os.path.join(save_path, f"description{suffix}.pkl"), 'wb') as f:
            pickle.dump(mini_desc, f)

        print(
            f"     - èŠ‚ç‚¹: {len(sorted_ents)} (å«æè¿°: {count}), è¾¹: {len(mini_triples)}")
        return old2new_ent

    map_1 = process_subgraph(target_ids_1, trip_1, ent_1, rel_1, attr_1, "1")
    map_2 = process_subgraph(target_ids_2, trip_2, ent_2, rel_2, attr_2, "2")

    # 4. ä¿å­˜å¯¹é½å¯¹ (åªä¿å­˜é‚£ 10 å¯¹ Core)
    print("ğŸ’¾ ä¿å­˜ ref_pairs (ä»… 10 å¯¹)...")
    with open(os.path.join(save_path, "ref_pairs"), 'w', encoding='utf-8') as f:
        for old_1, old_2 in core_pairs:
            f.write(f"{map_1[old_1]}\t{map_2[old_2]}\n")

    print(f"\nâœ… æ­¥éª¤ä¸€å®Œæˆï¼æ•°æ®é›†ä½äº {save_path}")


if __name__ == "__main__":
    run()
