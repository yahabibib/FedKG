# ğŸ“„ fl_core.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import collections
import copy
import config
from models import get_model_class
from tqdm import tqdm
import logging


class Client:
    def __init__(self, client_id, device, proxies, **kwargs):
        self.client_id = client_id
        self.device = device
        self.model_type = config.MODEL_ARCH

        # ä¿å­˜æœ¬åœ°çš„ä»£ç†å‰¯æœ¬ (å¯è®­ç»ƒ)
        self.proxies = proxies.clone().detach().to(self.device)
        self.proxies.requires_grad = True

        ModelClass = get_model_class(self.model_type)

        # --- å›¾æ¨¡å‹åˆå§‹åŒ– ---
        if self.model_type in ['gcn', 'decoupled']:
            self.adj = kwargs['adj']
            num_entities = kwargs['num_ent']
            local_bert_embs = kwargs['bert']

            self.model = ModelClass(
                num_entities=num_entities,
                feature_dim=config.GCN_DIM,
                hidden_dim=config.GCN_HIDDEN,
                output_dim=config.BERT_DIM,
                dropout=config.GCN_DROPOUT
            ).to(self.device)

            # å‡†å¤‡ SBERT æ•°æ® (ä½œä¸º Teacher)
            sbert_tensor = torch.zeros(num_entities, config.BERT_DIM)
            self.train_indices = []
            for ent_id, emb in local_bert_embs.items():
                if ent_id < num_entities:
                    sbert_tensor[ent_id] = emb
                    self.train_indices.append(ent_id)

            self.sbert_target = sbert_tensor.to(self.device)
            self.train_indices = torch.tensor(
                self.train_indices).to(self.device)

            logging.info(
                f"Client {client_id}: {len(self.train_indices)} anchors ready for Proxy Alignment.")

    def update_anchors(self, new_targets_dict):
        # ä¼ªæ ‡ç­¾é€»è¾‘ä¿æŒä¸å˜ï¼Œåªæ˜¯æ›´æ–° sbert_target
        count = 0
        for ent_id, new_emb in new_targets_dict.items():
            if ent_id < len(self.sbert_target):
                self.sbert_target[ent_id] = new_emb.to(self.device)
                count += 1
        mask = self.sbert_target.abs().sum(dim=1) > 1e-6
        self.train_indices = torch.nonzero(mask).squeeze().to(self.device)
        logging.info(
            f"    [{self.client_id}] Anchors Updated. Total: {len(self.train_indices)} (+{count})")

    def local_train(self, global_model_state, global_proxies, local_epochs, lr):
        """
        è¿”å›: (æ›´æ–°åçš„æ¨¡å‹å‚æ•°, æ›´æ–°åçš„ä»£ç†å‚æ•°, å¹³å‡Loss)
        """
        # 1. åŠ è½½å…¨å±€æ¨¡å‹å‚æ•°
        if global_model_state is not None:
            my_state = self.model.state_dict()
            for k, v in global_model_state.items():
                if "initial_features" in k:
                    continue
                if self.model_type == 'decoupled' and "struct_encoder" in k:
                    continue
                my_state[k] = v
            self.model.load_state_dict(my_state)

        # 2. åŠ è½½å…¨å±€ä»£ç†å‚æ•°
        if global_proxies is not None:
            self.proxies.data = global_proxies.to(self.device)

        # 3. è®­ç»ƒ
        if self.model_type in ['gcn', 'decoupled']:
            return self._train_proxy_alignment(local_epochs, lr)
        else:
            raise NotImplementedError("Proxy mode only supports GCN/Decoupled")

    def _train_proxy_alignment(self, epochs, lr):
        self.model.train()

        # 1. ä¼˜åŒ–å™¨ (ä¿æŒä¸å˜)
        optimizer = optim.Adam([
            {'params': self.model.parameters(), 'lr': lr},
            {'params': [self.proxies], 'lr': config.PROXY_LR}
        ])

        # 2. å®šä¹‰ä¸¤ä¸ª Loss
        # (A) ä¸» Loss: MarginRankingLoss (æ‰¾å›ä¸¢å¤±çš„ç²¾åº¦)
        criterion_rank = nn.MarginRankingLoss(margin=config.FL_MARGIN)
        # (B) è¾… Loss: KLDivLoss (ä¿ç•™åŠ¨æ€ä»£ç†çš„è°ƒèŠ‚èƒ½åŠ›)
        criterion_kl = nn.KLDivLoss(reduction='batchmean')

        temp = config.PROXY_TEMPERATURE
        # æ··åˆæƒé‡: ä¸»è¦æ˜¯ Rankingï¼ŒKL ä½œä¸ºè¾…åŠ©
        LAMBDA_KL = 0.05

        total_loss = 0.0

        for epoch in range(epochs):
            optimizer.zero_grad()

            # --- Forward ---
            gcn_out = self.model(self.adj)
            student_emb = gcn_out[self.train_indices]
            # SBERT ä»ç„¶æ˜¯ç¡¬é”šç‚¹
            teacher_emb = self.sbert_target[self.train_indices].detach()

            # --- Loss A: Margin Ranking (å¤åˆ»åŸæ–¹æ¡ˆçš„é€»è¾‘) ---
            # æ­£ä¾‹ç›¸ä¼¼åº¦
            pos_sim = F.cosine_similarity(student_emb, teacher_emb)

            # ç¡¬è´Ÿé‡‡æ · (Batch å†…)
            with torch.no_grad():
                sim_mat = torch.mm(F.normalize(student_emb, dim=1),
                                   F.normalize(teacher_emb, dim=1).T)
                sim_mat.fill_diagonal_(-2.0)
                hard_neg_indices = sim_mat.argmax(dim=1)

            neg_target = teacher_emb[hard_neg_indices]
            neg_sim = F.cosine_similarity(student_emb, neg_target)

            y_target = torch.ones_like(pos_sim)
            loss_rank = criterion_rank(pos_sim, neg_sim, y_target)

            # --- Loss B: Proxy KL Divergence (åŠ¨æ€ä»£ç†éƒ¨åˆ†) ---
            norm_proxies = F.normalize(self.proxies, dim=1)
            norm_student = F.normalize(student_emb, dim=1)
            norm_teacher = F.normalize(teacher_emb, dim=1)

            # Student åˆ†å¸ƒ
            student_logits = torch.mm(norm_student, norm_proxies.T) / temp
            student_log_probs = F.log_softmax(student_logits, dim=1)

            # Teacher åˆ†å¸ƒ
            with torch.no_grad():
                teacher_logits = torch.mm(norm_teacher, norm_proxies.T) / temp
                teacher_probs = F.softmax(teacher_logits, dim=1)

            loss_kl = criterion_kl(student_log_probs, teacher_probs)

            # --- Total Loss ---
            # ç»“åˆä¸¤è€…
            loss = loss_rank + (LAMBDA_KL * loss_kl)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        return self.model.state_dict(), self.proxies.detach().cpu(), total_loss / max(1, epochs)


class Server:
    def __init__(self, initial_proxies):
        self.device = config.DEVICE
        ModelClass = get_model_class(config.MODEL_ARCH)

        # åˆå§‹åŒ–å…¨å±€æ¨¡å‹
        self.global_model = ModelClass(
            1, config.GCN_DIM, config.GCN_HIDDEN, config.BERT_DIM, 0
        ).to(self.device)

        # åˆå§‹åŒ–å…¨å±€ä»£ç†
        self.global_proxies = initial_proxies.to(self.device)
        logging.info(
            f"Server initialized with {len(self.global_proxies)} dynamic proxies.")

    def get_global_model_state(self):
        return self.global_model.state_dict()

    def get_global_proxies(self):
        return self.global_proxies

    def aggregate(self, client_models, client_proxies):
        """
        åŒæ—¶èšåˆ æ¨¡å‹å‚æ•° å’Œ ä»£ç†å‚æ•°
        """
        # 1. èšåˆæ¨¡å‹ (FedAvg)
        avg_weights = collections.OrderedDict()

        # éå†å…¨å±€æ¨¡å‹çš„æ‰€æœ‰å‚æ•°é”®
        for key in self.global_model.state_dict().keys():
            # è¿‡æ»¤æ‰ç§æœ‰å±‚
            if "initial_features" in key:
                continue
            if config.MODEL_ARCH == 'decoupled' and "struct_encoder" in key:
                continue

            # æ”¶é›†å„å®¢æˆ·ç«¯çš„è¯¥å‚æ•°
            tensors = [s[key].to(self.device)
                       for s in client_models if key in s]

            if tensors:
                # ã€å…³é”®ä¿®å¤ã€‘æ£€æŸ¥æ•°æ®ç±»å‹
                if torch.is_floating_point(tensors[0]):
                    # æµ®ç‚¹æ•°ç›´æ¥æ±‚å¹³å‡
                    avg_weights[key] = torch.stack(tensors).mean(dim=0)
                else:
                    # æ•´æ•° (å¦‚ num_batches_tracked) éœ€è¦å…ˆè½¬ float å†è½¬å› long
                    avg_weights[key] = torch.stack(
                        tensors).float().mean(dim=0).long()

        # åŠ è½½èšåˆåçš„å‚æ•°åˆ°å…¨å±€æ¨¡å‹
        my_state = self.global_model.state_dict()
        my_state.update(avg_weights)
        self.global_model.load_state_dict(my_state)

        # 2. èšåˆä»£ç† (FedAvg)
        # ä»£ç†å‘é‡æœ¬èº«æ˜¯ floatï¼Œç›´æ¥æ±‚å¹³å‡å³å¯
        stacked_proxies = torch.stack(
            [p.to(self.device) for p in client_proxies])
        new_proxies = stacked_proxies.mean(dim=0)

        # è®¡ç®—ä»£ç†ç§»åŠ¨äº†å¤šå°‘
        diff = torch.norm(new_proxies - self.global_proxies).item()
        self.global_proxies = new_proxies

        return avg_weights, new_proxies, diff
