# ğŸ“„ fl_core.py
# ã€Relation-Aware ç‰ˆã€‘é€‚é… RelationGCNï¼Œç§»é™¤åŠ¨æ€ä»£ç†ï¼Œå›å½’æ ‡å‡†è”é‚¦æ¶æ„

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import collections
import config
from models import get_model_class
from tqdm import tqdm
import logging


class Client:
    def __init__(self, client_id, device, **kwargs):
        self.client_id = client_id
        self.device = device
        self.model_type = config.MODEL_ARCH

        ModelClass = get_model_class(self.model_type)

        # --- å›¾æ¨¡å‹åˆå§‹åŒ– (GCN / Decoupled) ---
        if self.model_type in ['gcn', 'decoupled']:
            # ğŸ”¥ [å…³é”®ä¿®æ”¹] æ¥æ”¶å›¾ç»“æ„ç´¢å¼•å’Œå…³ç³»è¯­ä¹‰
            self.edge_index = kwargs['edge_index'].to(self.device)
            self.edge_type = kwargs['edge_type'].to(self.device)

            num_entities = kwargs['num_ent']
            num_relations = kwargs['num_rel']  # åŸå§‹å…³ç³»æ•°é‡
            local_bert_embs = kwargs['bert']
            rel_sbert = kwargs.get('rel_sbert', None)  # å…³ç³» SBERT

            # åˆå§‹åŒ–æ¨¡å‹
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ ModelClass (GCN æˆ– Decoupled) å·²ç»é€‚é…äº† num_relations å‚æ•°
            self.model = ModelClass(
                num_entities=num_entities,
                num_relations=num_relations,   # ä¼ å…¥å…³ç³»æ•°
                feature_dim=config.GCN_DIM,
                hidden_dim=config.GCN_HIDDEN,
                output_dim=config.BERT_DIM,
                dropout=config.GCN_DROPOUT
            ).to(self.device)

            # ğŸ”¥ [å…³é”®ä¿®æ”¹] åˆå§‹åŒ–å…³ç³»åµŒå…¥
            # å¦‚æœæ¨¡å‹æœ‰è¿™ä¸ªæ–¹æ³• (RelationGCN åº”è¯¥æœ‰)ï¼Œå°±åˆå§‹åŒ–
            if hasattr(self.model, 'init_relation_embeddings') and rel_sbert is not None:
                self.model.init_relation_embeddings(rel_sbert)
            # å¦‚æœæ˜¯ Decoupledï¼Œå¯èƒ½éœ€è¦æ·±å…¥åˆ° self.model.struct_encoder é‡Œå»åˆå§‹åŒ–
            elif hasattr(self.model, 'struct_encoder') and rel_sbert is not None:
                # å‡è®¾ struct_encoder æ˜¯ä¸€ä¸ª ModuleListï¼Œæˆ–è€…å°±æ˜¯ RelationGCN
                # è¿™é‡Œåšä¸ªç®€å•çš„å°è¯•ï¼Œå¦‚æœä½ çš„ Decoupled å†™æ³•ä¸åŒï¼Œå¯èƒ½è¦å¾®è°ƒ
                for module in self.model.modules():
                    if hasattr(module, 'init_relation_embeddings'):
                        module.init_relation_embeddings(rel_sbert)
                        break

            # å‡†å¤‡ SBERT Target (é”šç‚¹)
            sbert_tensor = torch.zeros(num_entities, config.BERT_DIM)
            self.train_indices = []
            for ent_id, emb in local_bert_embs.items():
                if ent_id < num_entities:
                    sbert_tensor[ent_id] = emb
                    self.train_indices.append(ent_id)

            self.sbert_target = sbert_tensor.to(self.device)
            self.train_indices = torch.tensor(
                self.train_indices).to(self.device)

            logging.info(
                f"Client {self.client_id}: {len(self.train_indices)} anchors ready.")

        # --- Projection (TransE) åˆå§‹åŒ– (ä¿æŒå…¼å®¹) ---
        elif self.model_type == 'projection':
            local_transe_embs = kwargs['transe']
            local_bert_embs = kwargs['bert']
            self.model = ModelClass(
                input_dim=config.TRANSE_DIM, output_dim=config.BERT_DIM).to(self.device)
            self.train_data = []
            for ent_id, transe_emb in local_transe_embs.items():
                if ent_id in local_bert_embs:
                    self.train_data.append(
                        (transe_emb.to(device), local_bert_embs[ent_id].to(device)))

    def update_anchors(self, new_targets_dict):
        count = 0
        for ent_id, new_emb in new_targets_dict.items():
            if ent_id < len(self.sbert_target):
                self.sbert_target[ent_id] = new_emb.to(self.device)
                count += 1
        mask = self.sbert_target.abs().sum(dim=1) > 1e-6
        self.train_indices = torch.nonzero(mask).squeeze().to(self.device)
        logging.info(
            f"    [{self.client_id}] Anchors Updated. Total: {len(self.train_indices)} (+{count})")

    def local_train(self, global_model_state, local_epochs, batch_size, lr):
        # 1. åŠ è½½å…¨å±€å‚æ•°
        if global_model_state is not None:
            my_state = self.model.state_dict()
            for k, v in global_model_state.items():
                if "initial_features" in k:
                    continue
                if "relation_embeddings" in k:
                    continue  # å…³ç³»åµŒå…¥é€šå¸¸ä¹Ÿè§†ä¸ºç§æœ‰æˆ–åŠç§æœ‰ï¼Œè§†ç­–ç•¥è€Œå®š
                if self.model_type == 'decoupled' and "struct_encoder" in k:
                    continue

                # å…¼å®¹æ€§æ£€æŸ¥ï¼šç¡®ä¿ shape åŒ¹é…æ‰åŠ è½½
                if k in my_state and v.shape == my_state[k].shape:
                    my_state[k] = v
            self.model.load_state_dict(my_state, strict=False)

        # 2. è®­ç»ƒ
        if self.model_type in ['gcn', 'decoupled']:
            return self._train_graph_model(local_epochs, lr)
        elif self.model_type == 'projection':
            return self._train_projection(local_epochs, batch_size, lr)

    def _train_graph_model(self, epochs, lr):
        self.model.train()
        optimizer = optim.Adam(self.model.parameters(), lr=lr)
        criterion = nn.MarginRankingLoss(margin=config.FL_MARGIN)

        total_loss = 0.0

        for epoch in range(epochs):
            optimizer.zero_grad()

            # ğŸ”¥ [å…³é”®ä¿®æ”¹] Forward ä¼ å…¥ edge_index å’Œ edge_type
            # å…¼å®¹ Decoupled æ¶æ„ï¼šDecoupledModel.forward éœ€è¦æ¥æ”¶è¿™ä¿©å‚æ•°å¹¶ä¼ ç»™ struct_encoder
            output = self.model(self.edge_index, self.edge_type)

            out_batch = output[self.train_indices]
            target_batch = self.sbert_target[self.train_indices]

            pos_sim = F.cosine_similarity(out_batch, target_batch)

            # å›°éš¾è´Ÿé‡‡æ ·
            with torch.no_grad():
                sim_mat = torch.mm(F.normalize(out_batch, dim=1),
                                   F.normalize(target_batch, dim=1).T)
                sim_mat.fill_diagonal_(-2.0)
                hard_neg_indices = sim_mat.argmax(dim=1)

            neg_target = target_batch[hard_neg_indices]
            neg_sim = F.cosine_similarity(out_batch, neg_target)

            y = torch.ones_like(pos_sim)
            loss = criterion(pos_sim, neg_sim, y)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

            # ğŸ”¥ [æ–°å¢] å¼ºåˆ¶æ¸…ç†æ˜¾å­˜ç¼“å­˜ (é’ˆå¯¹ Mac MPS)
            if config.DEVICE.type == 'mps':
                torch.mps.empty_cache()

        return self.model.state_dict(), total_loss / max(1, epochs)

    def _train_projection(self, epochs, batch_size, lr):
        # ... (Projection é€»è¾‘ä¿æŒä¸å˜ï¼Œä¸ºäº†èŠ‚çœç¯‡å¹…çœç•¥) ...
        return self.model.state_dict(), 0.0


class Server:
    def __init__(self):
        self.device = config.DEVICE
        ModelClass = get_model_class(config.MODEL_ARCH)

        # åˆå§‹åŒ–å…¨å±€æ¨¡å‹ (ç”¨äºå‚æ•°èšåˆçš„å®¹å™¨)
        # æ³¨æ„ï¼šServer å…¶å®ä¸éœ€è¦çŸ¥é“å…³ç³»æ•°ï¼Œå› ä¸ºå®ƒåªèšåˆ MLP éƒ¨åˆ†
        # ä½†ä¸ºäº†åˆå§‹åŒ–ä¸æŠ¥é”™ï¼Œæˆ‘ä»¬éšä¾¿ä¼ ä¸ª 1
        if config.MODEL_ARCH in ['gcn', 'decoupled']:
            self.global_model = ModelClass(
                num_entities=1,
                num_relations=1,  # å ä½ç¬¦
                feature_dim=config.GCN_DIM,
                hidden_dim=config.GCN_HIDDEN,
                output_dim=config.BERT_DIM,
                dropout=0
            ).to(self.device)
        else:
            self.global_model = ModelClass(
                config.TRANSE_DIM, config.BERT_DIM).to(self.device)

    def get_global_model_state(self):
        return self.global_model.state_dict()

    def aggregate_models(self, client_model_states):
        if not client_model_states:
            return None

        avg_weights = collections.OrderedDict()

        # éå†å…¨å±€æ¨¡å‹ Key
        for key in self.global_model.state_dict().keys():
            # è¿‡æ»¤ç§æœ‰å±‚
            if "initial_features" in key:
                continue
            if "relation_embeddings" in key:
                continue  # å…³ç³»åµŒå…¥ä¸èšåˆ
            if config.MODEL_ARCH == 'decoupled' and "struct_encoder" in key:
                continue

            tensors = [s[key].to(self.device)
                       for s in client_model_states if key in s]
            if not tensors:
                continue

            # èšåˆé€»è¾‘ (å…¼å®¹ LongTensor)
            if torch.is_floating_point(tensors[0]):
                avg_weights[key] = torch.stack(tensors).mean(dim=0)
            else:
                avg_weights[key] = torch.stack(
                    tensors).float().mean(dim=0).long()

        my_state = self.global_model.state_dict()
        my_state.update(avg_weights)
        self.global_model.load_state_dict(my_state, strict=False)
        return avg_weights
