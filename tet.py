# ğŸ“„ step3_dual_eval.py
# æ­¥éª¤ä¸‰ï¼šå«å™ªå£°çš„å¯¹é½è¯„ä¼° (Alignment Evaluation with Noise)
# ç›®æ ‡ï¼šåœ¨å¤§é‡å¹²æ‰°é¡¹å­˜åœ¨çš„æƒ…å†µä¸‹ï¼Œå¯¹æ¯” [Pure] vs [Mech] vs [Polish] çš„å¯¹é½èƒ½åŠ›

import torch
import torch.nn.functional as F
import config
import data_loader
import precompute
import evaluate
import os
import pickle
import logging
import shutil
import random
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%H:%M:%S'
)


def load_custom_pkl(path):
    if not os.path.exists(path):
        return {}
    with open(path, 'rb') as f:
        return pickle.load(f)

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================


def get_noise_texts(model, noise_count=1000):
    """
    ä»å…¨é‡æ•°æ®ä¸­æå–å™ªéŸ³å®ä½“çš„ Embedding
    (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä» description2.pkl ä¸­éšæœºæŠ½ï¼Œæ¨¡æ‹Ÿ KG2 çš„å¹²æ‰°é¡¹)
    """
    logging.info(f"   ğŸ¦  æ­£åœ¨å‡†å¤‡ {noise_count} ä¸ªå™ªéŸ³å®ä½“...")

    # åŠ è½½å…¨é‡æè¿°
    pkl_path = config.BASE_PATH + "description2.pkl"
    if not os.path.exists(pkl_path):
        logging.warning("   âš ï¸ å…¨é‡æè¿°æ–‡ä»¶ç¼ºå¤±ï¼Œæ— æ³•æ³¨å…¥å™ªéŸ³ã€‚")
        return None

    with open(pkl_path, 'rb') as f:
        full_data = pickle.load(f)

    all_texts = []
    keys = list(full_data.keys())

    # éšæœºé‡‡æ ·
    if len(keys) > noise_count:
        keys = random.sample(keys, noise_count)

    for k in keys:
        # ç®€å•æ¸…æ´—
        text = str(full_data[k]).strip()
        if len(text) > 5:
            all_texts.append(text)

    # è®¡ç®—å™ªéŸ³å‘é‡
    logging.info(f"   ğŸ”„ è®¡ç®—å™ªéŸ³å‘é‡ ({len(all_texts)} æ¡)...")
    noise_embs = model.encode(
        all_texts, convert_to_tensor=True, show_progress_bar=False)
    return F.normalize(noise_embs, p=2, dim=1)


def local_fine_tune_sbert(model_path, text_map_1, text_map_2, train_pairs, save_path, batch_size=16, epochs=3):
    """ SBERT å¾®è°ƒé€»è¾‘ """
    logging.info(f"   ğŸ”§ [Fine-tune] Init model: {model_path}...")
    model = SentenceTransformer(model_path, device=config.DEVICE)
    model.train()

    train_examples = []
    for id1, id2 in train_pairs:
        t1 = text_map_1.get(id1, "")
        t2 = text_map_2.get(id2, "")
        if t1 and t2:
            train_examples.append(InputExample(texts=[t1, t2]))

    # æ•°æ®å¢å¼ºï¼šå¤åˆ¶å‡ ä»½ä»¥å¢åŠ  epoch å†…çš„ step æ•°
    if len(train_examples) < 50:
        train_examples = train_examples * 5

    logging.info(f"   ğŸ“¦ Training samples: {len(train_examples)}")

    loader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
    train_loss = losses.MultipleNegativesRankingLoss(model)

    if os.path.exists(save_path):
        shutil.rmtree(save_path)
    os.makedirs(save_path)

    model.fit(
        train_objectives=[(loader, train_loss)],
        epochs=epochs,
        warmup_steps=int(len(loader) * 0.1),
        show_progress_bar=True,
        output_path=save_path,
        optimizer_params={'lr': 2e-5}
    )

    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    return save_path

# ==========================================
# ğŸ§ª å®éªŒä¸»æµç¨‹
# ==========================================


def run_experiment(mode, noise_level=1000):
    print(f"\n{'='*40}")
    print(f"ğŸ§ª å®éªŒç»„: [{mode.upper()}] (å« {noise_level} å™ªéŸ³)")
    print(f"{'='*40}")

    demo_path = "data/demo_mini/zh_en/"

    # 1. åŠ è½½ Mini æ•°æ® (åŒ…å« 10 å¯¹æ ¸å¿ƒ + 40 å¯¹èƒŒæ™¯)
    ent_1, _ = data_loader.load_id_map(demo_path + "ent_ids_1")
    ent_2, _ = data_loader.load_id_map(demo_path + "ent_ids_2")
    ref_pairs = data_loader.load_alignment_pairs(
        demo_path + "ref_pairs")  # è¿™ 10 å¯¹ç”¨äºå¾®è°ƒå’Œæµ‹è¯•

    # 2. åŠ è½½æè¿° (ä¸åŒæ¨¡å¼åŠ è½½ä¸åŒæ–‡ä»¶)
    if mode == 'pure':
        # Pure æ¨¡å¼ç”¨åŸå§‹æè¿°
        t1 = load_custom_pkl(demo_path + "description1.pkl")
        t2 = load_custom_pkl(demo_path + "description2.pkl")
    elif mode == 'mech':
        t1 = load_custom_pkl(demo_path + "desc_mech_1.pkl")
        t2 = load_custom_pkl(demo_path + "desc_mech_2.pkl")
    else:  # polish
        t1 = load_custom_pkl(demo_path + "desc_polish_1.pkl")
        t2 = load_custom_pkl(demo_path + "desc_polish_2.pkl")

    if not t1:
        logging.error("âŒ æ•°æ®åŠ è½½å¤±è´¥")
        return 0.0

    # 3. å¾®è°ƒ SBERT (Pure æ¨¡å¼ä¸å¾®è°ƒï¼Œä½œä¸º Zero-shot Baseline)
    if mode == 'pure':
        logging.info("   â© Pure æ¨¡å¼è·³è¿‡å¾®è°ƒ (Zero-shot)...")
        model_path = config.BERT_MODEL_NAME
    else:
        save_path = f"fine_tuned_models/demo_{mode}"
        model_path = local_fine_tune_sbert(
            config.BERT_MODEL_NAME, t1, t2, ref_pairs, save_path, epochs=5  # å¾®è°ƒ 5 è½®
        )

    # 4. å‡†å¤‡è¯„ä¼° Embedding
    # åŠ è½½æ¨¡å‹ç”¨äºæ¨ç†
    eval_model = SentenceTransformer(model_path, device=config.DEVICE)
    eval_model.eval()

    # A. è®¡ç®— Query (KG1 æ ¸å¿ƒå®ä½“)
    query_texts = [t1[p[0]] for p in ref_pairs]
    query_embs = eval_model.encode(
        query_texts, convert_to_tensor=True, show_progress_bar=False)
    query_embs = F.normalize(query_embs, p=2, dim=1)

    # B. è®¡ç®— Target (KG2 æ ¸å¿ƒå®ä½“)
    target_ids = [p[1] for p in ref_pairs]
    target_texts = [t2[tid] for tid in target_ids]
    target_embs = eval_model.encode(
        target_texts, convert_to_tensor=True, show_progress_bar=False)
    target_embs = F.normalize(target_embs, p=2, dim=1)

    # C. è®¡ç®— Noise (KG2 å¤–éƒ¨å™ªéŸ³)
    noise_embs = get_noise_texts(eval_model, noise_count=noise_level)

    # 5. åˆå¹¶å€™é€‰æ±  (Target + Noise)
    if noise_embs is not None:
        candidate_embs = torch.cat([target_embs, noise_embs], dim=0)
    else:
        candidate_embs = target_embs

    # 6. è®¡ç®— Hits@1
    # Similarity Matrix: [Num_Query, Num_Candidates]
    sim_mat = torch.mm(query_embs, candidate_embs.T)

    hits1 = 0
    # å¯¹äºç¬¬ i ä¸ª Queryï¼Œæ­£ç¡®çš„ç­”æ¡ˆå°±åœ¨ candidate_embs çš„ç¬¬ i ä¸ªä½ç½®
    # (å› ä¸ºæˆ‘ä»¬æ˜¯æŒ‰é¡ºåºæ‹¼æ¥ target çš„ï¼Œnoise æ‹¼åœ¨åé¢)
    for i in range(len(query_embs)):
        scores = sim_mat[i]
        best_idx = torch.argmax(scores).item()

        if best_idx == i:  # å‘½ä¸­æ­£ç¡®ç­”æ¡ˆ
            hits1 += 1

    acc = hits1 / len(query_embs) * 100
    logging.info(f"   ğŸ¯ Alignment Hits@1: {acc:.2f}%")

    del eval_model
    return acc


if __name__ == "__main__":
    # è®¾ç½®å™ªéŸ³ç­‰çº§
    NOISE_LEVEL = 2000

    print(f"ğŸš€ å¯åŠ¨æŠ—å™ªå¯¹é½å®éªŒ (Noise={NOISE_LEVEL})")

    # 1. Pure (åŸºå‡†)
    score_pure = run_experiment('pure', NOISE_LEVEL)

    # 2. Mech (æœºæ¢°)
    score_mech = run_experiment('mech', NOISE_LEVEL)

    # 3. Polish (æ¶¦è‰²)
    score_poli = run_experiment('polish', NOISE_LEVEL)

    print("\n" + "="*60)
    print(f"ğŸ† æœ€ç»ˆå¯¹é½ç»“æœå¯¹æ¯” (Hits@1)")
    print(f"{'='*60}")
    print(f"{'Mode':<10} | {'Hits@1':<10} | {'Gap vs Pure':<15}")
    print("-" * 50)
    print(f"{'Pure':<10} | {score_pure:.2f}%     | -")
    print(f"{'Mech':<10} | {score_mech:.2f}%     | {score_mech-score_pure:+.2f}%")
    print(f"{'Polish':<10} | {score_poli:.2f}%     | {score_poli-score_pure:+.2f}%")
    print("-" * 50)

    if score_poli > score_mech and score_poli >= score_pure:
        print("âœ… éªŒè¯æˆåŠŸï¼æ¶¦è‰² + å¾®è°ƒ åœ¨é«˜å™ªéŸ³ä¸‹è¡¨ç°æœ€ä½³ã€‚")
    else:
        print("âš ï¸ éªŒè¯éœ€åˆ†æï¼šå¯èƒ½æ˜¯å¾®è°ƒè¿‡æ‹Ÿåˆï¼Œæˆ–å™ªéŸ³å¤ªå¼ºæ·¹æ²¡è¯­ä¹‰ã€‚")
    print("="*60)
