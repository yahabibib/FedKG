# src/llm/finetuner.py
import logging
import os
import shutil
from typing import List, Tuple
import torch
from torch.utils.data import DataLoader
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

from src.utils.config import Config


class SBERTFinetuner:
    """
    Stage 1.2: SBERT å¾®è°ƒå™¨
    ä½¿ç”¨å¯¹æ¯”å­¦ä¹  (InfoNCE Loss) å°† LLM ç”Ÿæˆçš„ç»“æ„åŒ–æ–‡æœ¬æ³¨å…¥ SBERTã€‚
    """

    def __init__(self, config: Config):
        self.cfg = config
        self.logger = logging.getLogger("SBERTFinetuner")
        self.device = self.cfg.device

    def fine_tune(self,
                  train_pairs: List[Tuple[str, str]],
                  output_path: str,
                  epochs: int = 3,
                  batch_size: int = 16,
                  freeze_layers: int = 10):
        """
        æ‰§è¡Œå¾®è°ƒ
        :param train_pairs: List of (anchor_text, positive_text)
                            ä¾‹å¦‚ [(åŸå§‹æè¿°, æ¶¦è‰²åçš„ç»“æ„æ–‡æœ¬), ...]
        """
        self.logger.info(f"ğŸš€ å¼€å§‹ SBERT å¾®è°ƒï¼ŒåŸºåº§: {self.cfg.sbert_model_path}")
        self.logger.info(
            f"   æ ·æœ¬æ•°: {len(train_pairs)} | Epochs: {epochs} | Batch: {batch_size}")

        # 1. åŠ è½½æ¨¡å‹
        model = SentenceTransformer(
            self.cfg.sbert_model_path, device=str(self.device))

        # 2. å†»ç»“åº•å±‚å‚æ•° (Layer Freezing) - é˜²æ­¢ç¾éš¾æ€§é—å¿˜
        self._freeze_layers(model, freeze_layers)

        # 3. å‡†å¤‡æ•°æ®
        train_examples = [
            InputExample(texts=[t1, t2]) for t1, t2 in train_pairs
            if len(t1) > 5 and len(t2) > 5  # ç®€å•è¿‡æ»¤çŸ­æ–‡æœ¬
        ]

        train_dataloader = DataLoader(
            train_examples, shuffle=True, batch_size=batch_size)

        # 4. å®šä¹‰ Loss (Contrastive Loss)
        # MultipleNegativesRankingLoss è‡ªåŠ¨ä½¿ç”¨ batch å†…çš„å…¶ä»–æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
        train_loss = losses.MultipleNegativesRankingLoss(model)

        # 5. å¼€å§‹è®­ç»ƒ
        if os.path.exists(output_path):
            self.logger.warning(f"ç›®å½•å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–: {output_path}")
            shutil.rmtree(output_path)

        model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            epochs=epochs,
            warmup_steps=int(len(train_dataloader) * 0.1),
            show_progress_bar=True,
            output_path=output_path,
            optimizer_params={'lr': 2e-5}
        )

        self.logger.info(f"âœ… SBERT å¾®è°ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜è‡³: {output_path}")

        # æ¸…ç†å†…å­˜
        del model
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if torch.backends.mps.is_available():
            torch.mps.empty_cache()

    def _freeze_layers(self, model: SentenceTransformer, num_layers_to_freeze: int):
        """
        å†»ç»“ Transformer çš„å‰ N å±‚
        """
        auto_model = model._first_module().auto_model

        # 1. å†»ç»“ Embedding å±‚
        for param in auto_model.embeddings.parameters():
            param.requires_grad = False

        # 2. å†»ç»“ Encoder å±‚
        if hasattr(auto_model, 'encoder') and hasattr(auto_model.encoder, 'layer'):
            layers = auto_model.encoder.layer
            total_layers = len(layers)

            # ç¡®ä¿ä¸å†»ç»“æ‰€æœ‰å±‚
            freeze_limit = min(num_layers_to_freeze, total_layers - 1)

            for i in range(freeze_limit):
                for param in layers[i].parameters():
                    param.requires_grad = False

            self.logger.info(
                f"ğŸ§Š å·²å†»ç»“å‰ {freeze_limit}/{total_layers} å±‚ Transformer å‚æ•°ã€‚")
        else:
            self.logger.warning("âš ï¸ æ— æ³•è¯†åˆ«æ¨¡å‹ç»“æ„ï¼Œæœªæ‰§è¡Œå±‚å†»ç»“ã€‚")
