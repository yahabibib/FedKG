import torch
import gc
import logging
import os

log = logging.getLogger(__name__)


class DeviceManager:
    """
    Êô∫ËÉΩËÆæÂ§áÁÆ°ÁêÜÂô®Ôºö
    1. Ëá™Âä®ÈÄÇÈÖç MPS/CUDA/CPU
    2. Êèê‰æõÁªü‰∏ÄÁöÑÊòæÂ≠òÊ∏ÖÁêÜÊé•Âè£ (clean_memory)
    3. ÁÆ°ÁêÜ Offloading Á≠ñÁï•Áä∂ÊÄÅ
    """

    def __init__(self, cfg_system):
        self.cfg = cfg_system
        self.device = self._init_main_device()
        self._setup_env()

    def _init_main_device(self):
        """Ê†πÊçÆÈÖçÁΩÆÂíåÁ°¨‰ª∂Áé∞Áä∂ÂàùÂßãÂåñ‰∏ªËÆ°ÁÆóËÆæÂ§á"""
        req = self.cfg.device.lower()

        # 1. Â∞ùËØï CUDA
        if req == "cuda":
            if torch.cuda.is_available():
                return torch.device("cuda")
            log.warning("‚ö†Ô∏è Config requested CUDA but not available.")

        # 2. Â∞ùËØï MPS (Mac)
        if req == "mps":
            if torch.backends.mps.is_available():
                return torch.device("mps")
            log.warning("‚ö†Ô∏è Config requested MPS but not available.")

        # 3. ÂõûÈÄÄ
        log.info(f"Using fallback device: {self.cfg.fallback_device}")
        return torch.device(self.cfg.fallback_device)

    def _setup_env(self):
        """ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè‰ºòÂåñ"""
        if self.device.type == 'mps':
            # Mac ÊòæÂ≠ò‰ºòÂåñÂÖ≥ÈîÆÁéØÂ¢ÉÂèòÈáèÔºåËÆæ‰∏∫ 0.0 Ëß£Èô§‰∏äÈôêÈôêÂà∂ÔºåÈò≤Ê≠¢ËøáÊó©Êä•Èîô
            os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
            log.info(
                "üçé MPS Mode Detected: Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0")

    @property
    def main_device(self):
        """ËøîÂõûÁî®‰∫éËÆ°ÁÆóÁöÑËÆæÂ§á (GPU/MPS)"""
        return self.device

    @property
    def cpu_device(self):
        """ËøîÂõûÁî®‰∫éÂ≠òÂÇ®ÁöÑËÆæÂ§á"""
        return torch.device("cpu")

    def is_offload_enabled(self):
        """ÊòØÂê¶ÂêØÁî®Âç∏ËΩΩÁ≠ñÁï•"""
        return self.cfg.memory.offload_to_cpu

    def clean_memory(self):
        """
        Âº∫Âà∂ÂûÉÂúæÂõûÊî∂ÂíåÊòæÂ≠òÈáäÊîæ„ÄÇ
        Âú® Mac ‰∏äÔºåËøôÂØπ‰∫éÈò≤Ê≠¢ OOM Ëá≥ÂÖ≥ÈáçË¶Å„ÄÇ
        """
        if self.device.type == 'mps':
            torch.mps.empty_cache()
        elif self.device.type == 'cuda':
            torch.cuda.empty_cache()

        # Python Â±ÇÁöÑÂûÉÂúæÂõûÊî∂
        gc.collect()

    def get_safe_batch_size(self, requested_batch_size):
        """
        Ê£ÄÊü• task ËØ∑Ê±ÇÁöÑ batch_size ÊòØÂê¶Ë∂ÖËøá‰∫Ü system ÂÆö‰πâÁöÑÂÆâÂÖ®‰∏äÈôê
        """
        limit = self.cfg.get("max_batch_size", None)
        if limit and requested_batch_size > limit:
            log.warning(
                f"‚ö†Ô∏è Requested batch_size {requested_batch_size} exceeds system limit {limit}. Clamping to {limit}.")
            return limit
        return requested_batch_size
