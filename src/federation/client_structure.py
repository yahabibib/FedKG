import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sentence_transformers import SentenceTransformer
import logging
from tqdm import tqdm

from src.models.decoupled import DecoupledModel
from src.utils.graph import build_adjacency_matrix
from src.utils.device_manager import DeviceManager

log = logging.getLogger(__name__)


class ClientStructure:
    def __init__(self, client_id, cfg, dataset, device_manager: DeviceManager):
        self.client_id = client_id
        self.cfg = cfg
        self.dataset = dataset
        self.dm = device_manager
        self.device = self.dm.main_device

        # 1. æ„å»ºé‚»æ¥çŸ©é˜µ (CPU)
        self.adj = build_adjacency_matrix(
            dataset.triples,
            dataset.num_entities,
            device='cpu'
        )

        # 2. åŠ è½½ Frozen SBERT (CPU)
        sbert_path = cfg.task.sbert_checkpoint
        log.info(f"[{client_id}] Loading Frozen SBERT from: {sbert_path}")
        self.sbert = SentenceTransformer(sbert_path, device='cpu')
        self.sbert.eval()

        # 3. é¢„è®¡ç®— SBERT Anchors
        # å»ºè®®è¿™é‡ŒåŠ ä¸€ä¸ªç®€å•çš„ç¼“å­˜æ£€æµ‹ï¼Œé¿å…æ¯æ¬¡é‡å¯éƒ½ç®—ä¸€é (å¯é€‰ä¼˜åŒ–)
        self.anchor_embeddings = self._precompute_anchors()

        # 4. åˆå§‹åŒ–æ¨¡å‹
        self.model = DecoupledModel(cfg.task.model, dataset.num_entities)
        self.train_indices = torch.arange(dataset.num_entities)

    def _precompute_anchors(self):
        # ... (ä¿æŒåŸæœ‰çš„é¢„è®¡ç®—é€»è¾‘ä¸å˜) ...
        # ä¸ºèŠ‚çœç¯‡å¹…çœç•¥ï¼Œä¿æŒä½ ä¹‹å‰çš„ä»£ç å³å¯
        log.info(f"[{self.client_id}] Pre-computing semantic anchors...")
        ids = self.dataset.ids
        texts = self.dataset.get_text_list(ids, mode='desc')
        self.sbert.to(self.device)
        with torch.no_grad():
            embs = self.sbert.encode(
                texts,
                batch_size=self.dm.get_safe_batch_size(64),
                convert_to_tensor=True,
                show_progress_bar=True,
                device=self.device
            )
        self.sbert.to('cpu')
        self.dm.clean_memory()
        return embs.cpu()

    def update_anchors(self, indices, new_embeddings):
        """
        [å…³é”®ä¿®å¤] æ›´æ–°æœ¬åœ°é”šç‚¹ (Self-training)
        """
        # ç¡®ä¿ new_embeddings åœ¨ CPU (å› ä¸º self.anchor_embeddings åœ¨ CPU)
        if new_embeddings.device.type != 'cpu':
            new_embeddings = new_embeddings.cpu()

        # indices æ— è®ºä¼ è¿›æ¥æ˜¯ä»€ä¹ˆï¼Œéƒ½è½¬æˆ tensor ç”¨äºç´¢å¼•
        if not torch.is_tensor(indices):
            indices = torch.tensor(indices)
        if indices.device.type != 'cpu':
            indices = indices.cpu()

        # åŸåœ°æ›´æ–° (In-place update)
        self.anchor_embeddings[indices] = new_embeddings
        # log.info(f"[{self.client_id}] Anchors updated for {len(indices)} entities.")

    def train(self, custom_epochs=None):
        """
        è®­ç»ƒ GCN
        :param custom_epochs: å¦‚æœä¼ å…¥ï¼Œåˆ™è¦†ç›– config ä¸­çš„ local_epochs
        """
        # 1. ç¡®å®š Epochs
        epochs = custom_epochs if custom_epochs is not None else self.cfg.task.federated.local_epochs

        self.model.to(self.device)
        self.model.train()

        optimizer = optim.Adam(self.model.parameters(),
                               lr=self.cfg.task.federated.lr)
        criterion = nn.MarginRankingLoss(margin=self.cfg.task.federated.margin)
        batch_size = self.dm.get_safe_batch_size(
            self.cfg.task.federated.batch_size)

        n_samples = len(self.train_indices)
        total_loss = 0.0

        # --- æ—©åœç­–ç•¥å‚æ•° (Early Stopping Config) ---
        stop_threshold = 0.08  # å½“ loss ä½äºè¿™ä¸ªå€¼æ—¶å¼€å§‹ç›‘æµ‹
        patience = 3           # å®¹å¿å‡ æ¬¡ä¸ä¸‹é™
        min_delta = 0.005      # æœ€å°ä¸‹é™å¹…åº¦
        early_stop_counter = 0
        prev_epoch_loss = float('inf')

        # è¿›åº¦æ¡
        pbar_epoch = range(epochs)

        for epoch in pbar_epoch:
            # Shuffle
            perm = torch.randperm(n_samples)
            epoch_loss_sum = 0.0
            steps = 0

            # Batch Loop (ä¸ºäº†æ—¥å¿—ç®€æ´ï¼Œè¿™é‡Œä¸ç»™æ¯ä¸ªbatchéƒ½æ‰“è¿›åº¦æ¡äº†ï¼Œåªæ˜¾ç¤ºEpochè¿›åº¦)
            for i in range(0, n_samples, batch_size):
                idx = perm[i: i+batch_size]
                batch_ids = self.train_indices[idx].to(self.device)

                # A. GCN Forward
                output_emb = self.model(self.adj)
                struct_batch = output_emb[batch_ids]

                # B. Target (SBERT/Pseudo)
                target_batch = self.anchor_embeddings[batch_ids.cpu()].to(
                    self.device)

                # C. Loss Calculation
                pos_sim = F.cosine_similarity(struct_batch, target_batch)

                # Hard Negative Mining
                with torch.no_grad():
                    sim_mat = torch.mm(F.normalize(
                        struct_batch), F.normalize(target_batch).T)
                    sim_mat.fill_diagonal_(-2.0)
                    hard_neg_idx = sim_mat.argmax(dim=1)

                neg_target = target_batch[hard_neg_idx]
                neg_sim = F.cosine_similarity(struct_batch, neg_target)

                y = torch.ones_like(pos_sim)
                loss = criterion(pos_sim, neg_sim, y)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss_sum += loss.item()
                steps += 1

            # --- Epoch End Logic ---
            avg_loss = epoch_loss_sum / max(1, steps)

            # è¿™é‡Œçš„ print å¯ä»¥æ ¹æ®å–œå¥½æ”¹ä¸º tqdm.set_postfix
            # log.info(f"[{self.client_id}] Ep {epoch+1}/{epochs} | Loss: {avg_loss:.4f}")

            # --- è‡ªåŠ¨æ—©åœæ£€æŸ¥ ---
            if avg_loss < stop_threshold:
                # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ˜¾è‘—ä¸‹é™
                if (prev_epoch_loss - avg_loss) < min_delta:
                    early_stop_counter += 1
                    if early_stop_counter >= patience:
                        log.info(
                            f"   ğŸ›‘ [{self.client_id}] Early stopping at Epoch {epoch+1} (Loss={avg_loss:.4f})")
                        total_loss = avg_loss  # æ›´æ–°ä¸ºå½“å‰loss
                        break
                else:
                    early_stop_counter = 0  # Loss è¿˜åœ¨é™ï¼Œé‡ç½®è®¡æ•°å™¨

            prev_epoch_loss = avg_loss
            total_loss = avg_loss

        # 2. æ¸…ç†
        if self.dm.is_offload_enabled():
            self.model.to('cpu')
            self.dm.clean_memory()

        return self.model.get_shared_state_dict(), total_loss

    def get_embeddings(self):
        """æ¨ç†ï¼šè·å–æœ€ç»ˆçš„ç»“æ„ Embedding (ç”¨äºè¯„ä¼°)"""
        self.model.to(self.device)
        self.model.eval()
        with torch.no_grad():
            embs = self.model(self.adj)
        self.model.to('cpu')
        return embs.cpu()
