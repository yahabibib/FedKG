# ğŸ“„ step2_local_llm.py
# ã€æ‰¹é‡åŠ é€Ÿç‰ˆã€‘åˆ©ç”¨ Batch Inference æ¦¨å¹² CPU æ€§èƒ½
# 1. æ”¯æŒ Batch Size > 1 (å»ºè®®è®¾ä¸º 4-8ï¼Œè§†å†…å­˜è€Œå®š)
# 2. ä¿æŒæ‰€æœ‰é€»è¾‘ä¸€è‡´ (å¼ºçº¦æŸ Prompt + æ™ºèƒ½æ‘˜è¦)

import os
import json
import pickle
import torch
import config
import data_loader
from tqdm import tqdm
import re
import time
from transformers import AutoModelForCausalLM, AutoTokenizer
from collections import defaultdict
from torch.utils.data import Dataset, DataLoader

# ==========================================
# ğŸ”§ æ¨¡å‹é…ç½®
# ==========================================
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
DEVICE = "cpu"

# ã€å…³é”®ã€‘æ‰¹é‡å¤§å°
# å»ºè®®ä» 4 å¼€å§‹è¯•ï¼Œå¦‚æœå†…å­˜å¤Ÿå¤§å¯ä»¥åŠ åˆ° 8 æˆ– 16
# CPU æ¨ç†è™½ç„¶å¹¶è¡Œèƒ½åŠ›ä¸å¦‚ GPUï¼Œä½† Batching ä¾ç„¶èƒ½å‡å°‘ Python å¾ªç¯å¼€é”€
BATCH_SIZE = 16

# é…ç½®æ–‡ä»¶è·¯å¾„
PROMPT_FILE = "prompts.json"
PROGRESS_FILE_1 = "data/dbp15k/zh_en/progress_kg1.jsonl"
PROGRESS_FILE_2 = "data/dbp15k/zh_en/progress_kg2.jsonl"
FINAL_PKL_1 = "data/dbp15k/zh_en/desc_polish_1.pkl"
FINAL_PKL_2 = "data/dbp15k/zh_en/desc_polish_2.pkl"
# ==========================================

print(f"{'='*60}")
print(f"ğŸ¤– å…¨é‡æ•°æ®æ¶¦è‰² (Batch Speedup Mode)")
print(f"   Model: {MODEL_ID}")
print(f"   Batch Size: {BATCH_SIZE}")
print(f"{'='*60}")

# --- 0. åŠ è½½ Prompt ---
if not os.path.exists(PROMPT_FILE):
    print(f"âŒ ç¼ºå°‘ {PROMPT_FILE} æ–‡ä»¶ï¼")
    exit()
with open(PROMPT_FILE, 'r', encoding='utf-8') as f:
    PROMPTS = json.load(f)

# --- 1. åŠ è½½æ¨¡å‹ ---
try:
    print(f"ğŸ“¥ Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)

    # ã€å…³é”®ã€‘æ‰¹é‡ç”Ÿæˆå¿…é¡»è®¾ç½® padding_side='left'
    tokenizer.padding_side = 'left'
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.float32,
        device_map=DEVICE,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… Model loaded.")
except Exception as e:
    print(f"âŒ Load failed: {e}")
    exit()

# --- Dataset ç±» ---


class PromptDataset(Dataset):
    def __init__(self, items):
        self.items = items  # List of (eid, prompt, mech_text, base_desc)

    def __len__(self):
        return len(self.items)

    def __getitem__(self, idx):
        return self.items[idx]


def collate_fn(batch):
    # batch is list of tuples
    eids, prompts, mechs, descs = zip(*batch)
    return eids, prompts, mechs, descs

# --- æ‰¹é‡æ¨ç†å‡½æ•° ---


def batch_generate(prompts, system_prompt):
    # æ„é€ å¯¹è¯æ ¼å¼
    batch_texts = []
    for p in prompts:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": p}
        ]
        text = tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True)
        batch_texts.append(text)

    # Tokenize
    inputs = tokenizer(
        batch_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    ).to(DEVICE)

    # Generate
    with torch.no_grad():
        generated_ids = model.generate(
            inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=200,
            do_sample=False
        )

    # Decode
    # åªæå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†
    outputs = []
    input_len = inputs.input_ids.shape[1]
    for i, gen_ids in enumerate(generated_ids):
        # è£å‰ªæ‰è¾“å…¥éƒ¨åˆ†
        new_ids = gen_ids[input_len:]
        response = tokenizer.decode(new_ids, skip_special_tokens=True).strip()
        outputs.append(response)

    return outputs

# --- è¾…åŠ©å‡½æ•° ---


def clean_name(uri):
    if not isinstance(uri, str):
        return str(uri)
    name = uri.split('/')[-1].replace('_',
                                      ' ').replace('<', '').replace('>', '')
    name = re.sub(r'(?<!^)(?=[A-Z])', ' ', name).lower()
    return name.strip()


def get_smart_summary(text):
    if not text:
        return ""
    text = re.sub(r"\(.*?\)", "", text).replace("ï¼ˆ", "").replace("ï¼‰", "")
    if "ã€‚" in text:
        sentences = text.split("ã€‚")
    else:
        sentences = text.split(". ")
    summary = sentences[0].strip()
    if len(summary) < 10 and len(sentences) > 1:
        summary += "ï¼Œ" + sentences[1].strip()
    return summary[:80]

# --- è¿›åº¦ç®¡ç† ---


class ProgressManager:
    def __init__(self, log_file):
        self.log_file = log_file
        self.processed = {}
        self.load()

    def load(self):
        if not os.path.exists(self.log_file):
            return
        print(f"   ğŸ”„ Loading progress...")
        with open(self.log_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    item = json.loads(line)
                    self.processed[item['id']] = item['text']
                except:
                    pass
        print(f"   âœ… Resuming: {len(self.processed)} items done.")

    def is_done(self, eid):
        return eid in self.processed

    def save_batch(self, batch_results):
        with open(self.log_file, 'a', encoding='utf-8') as f:
            for eid, text in batch_results:
                f.write(json.dumps({'id': eid, 'text': text},
                        ensure_ascii=False) + "\n")
                self.processed[eid] = text

# --- 3. å¤„ç†é€»è¾‘ ---


def process_kg(kg_name, ent_map, rel_map, triples, attr_map, progress_file, final_pkl, lang_code):
    print(f"\nğŸš€ Processing {kg_name} ({len(ent_map)} entities)...")
    pm = ProgressManager(progress_file)

    prompt_config = PROMPTS[lang_code]
    system_prompt = prompt_config["system"]
    user_template = prompt_config["user_template"]

    # é¢„å¤„ç†ï¼šæ„å»ºé‚»æ¥è¡¨
    adj = defaultdict(list)
    for h, r, t in triples:
        adj[h].append((r, t))

    # 1. å‡†å¤‡å¾…å¤„ç†é˜Ÿåˆ— (è·³è¿‡å·²å®Œæˆçš„)
    pending_items = []
    mech_dict = {}   # ç”¨äºæœ€åæ±‡æ€»
    polish_dict = {}  # ç”¨äºæœ€åæ±‡æ€»

    # å…ˆæŠŠå·²å®Œæˆçš„åŠ è½½è¿›æ¥
    polish_dict.update(pm.processed)

    print("   Preparing task queue...")
    for eid, uri in ent_map.items():
        name = clean_name(uri)
        base_desc = attr_map.get(eid, "")

        # å¦‚æœå·²ç»å¤„ç†è¿‡ï¼Œä¸”æˆ‘ä»¬åœ¨ mech_dict éœ€è¦ç•™æ¡£ï¼Œè¿™é‡Œä¹Ÿè¦ç”Ÿæˆ mech
        neighbors = adj.get(eid, [])[:5]
        simple_triples = []
        rich_triples = []

        for r, t in neighbors:
            r_n = clean_name(rel_map.get(r, "rel"))
            t_n = clean_name(ent_map.get(t, "ent"))
            t_ctx = get_smart_summary(attr_map.get(t, ""))

            if t_ctx:
                rich_item = f"- å…³ç³»: {r_n} -> å¯¹è±¡: {t_n} (èƒŒæ™¯: {t_ctx})"
            else:
                rich_item = f"- å…³ç³»: {r_n} -> å¯¹è±¡: {t_n}"

            rich_triples.append(rich_item)
            simple_triples.append(f"{r_n}: {t_n}")

        # ç”Ÿæˆæœºæ¢°ç‰ˆ (Baseline)
        if simple_triples:
            mech_text = f"{base_desc} [SEP] Structure: {'; '.join(simple_triples)}"
        else:
            mech_text = base_desc
        mech_dict[eid] = mech_text

        # å¦‚æœå·²å¤„ç†ï¼Œè·³è¿‡åŠ å…¥é˜Ÿåˆ—
        if pm.is_done(eid):
            continue

        # å¦‚æœæ— ç»“æ„ï¼Œç›´æ¥ä¿å­˜ç»“æœ
        if not simple_triples:
            pm.save_batch([(eid, base_desc)])  # å®æ—¶ä¿å­˜
            continue

        # åŠ å…¥å¾…å¤„ç†é˜Ÿåˆ—
        data_str = "\n".join(rich_triples)
        prompt = user_template.format(name=name, data_str=data_str)
        pending_items.append((eid, prompt, mech_text, base_desc))

    print(f"   âš¡ï¸ Pending tasks: {len(pending_items)}")
    if not pending_items:
        print("   âœ… All done.")
        return mech_dict, polish_dict

    # 2. æ‰¹é‡æ¨ç†
    dataset = PromptDataset(pending_items)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE,
                            collate_fn=collate_fn, shuffle=False)

    pbar = tqdm(dataloader, desc="Batch Infer")
    for batch_eids, batch_prompts, batch_mechs, batch_descs in pbar:

        # æ‰¹é‡ç”Ÿæˆ
        batch_outputs = batch_generate(batch_prompts, system_prompt)

        # åå¤„ç†ä¸ä¿å­˜
        results_to_save = []

        for i, raw_out in enumerate(batch_outputs):
            eid = batch_eids[i]
            name = clean_name(ent_map[eid])  # é‡æ–°è·å–åå­—ç”¨äºæ ¡éªŒ

            polished = raw_out.replace(
                "Output:", "").replace("ç»“æœ:", "").strip()

            # ç®€å•æ ¡éªŒ
            if len(polished) < len(name) + 5:
                polished = batch_mechs[i].split("Structure:")[-1].strip()  # å›é€€

            final_text = f"{batch_descs[i]} [SEP] {polished}"
            results_to_save.append((eid, final_text))

            # ç›‘æ§
            # if i == 0: # æ¯ä¸ª Batch æ‰“å°ç¬¬ä¸€æ¡
            #    tqdm.write(f"   [Sample] {final_text[-50:]}...")

        pm.save_batch(results_to_save)

        # æ›´æ–°å†…å­˜ä¸­çš„å­—å…¸
        for eid, text in results_to_save:
            polish_dict[eid] = text

    # 3. å¯¼å‡º
    print(f"ğŸ’¾ Exporting to {final_pkl}...")
    with open(final_pkl, 'wb') as f:
        pickle.dump(pm.processed, f)
    print(f"âœ… {kg_name} Finished!")

    return mech_dict, pm.processed


def run():
    print("\nğŸ“š Loading Data...")
    ent_1, _ = data_loader.load_id_map(config.BASE_PATH + "ent_ids_1")
    ent_2, _ = data_loader.load_id_map(config.BASE_PATH + "ent_ids_2")
    rel_1, _ = data_loader.load_id_map(config.BASE_PATH + "rel_ids_1")
    rel_2, _ = data_loader.load_id_map(config.BASE_PATH + "rel_ids_2")
    trip_1 = data_loader.load_triples(config.BASE_PATH + "triples_1")
    trip_2 = data_loader.load_triples(config.BASE_PATH + "triples_2")

    attr_1 = data_loader.load_pickle_descriptions(
        config.BASE_PATH + "description1.pkl", (ent_1, {}))
    attr_2 = data_loader.load_pickle_descriptions(
        config.BASE_PATH + "description2.pkl", (ent_2, {}))

    m1, p1 = process_kg("KG1", ent_1, rel_1, trip_1, attr_1,
                        PROGRESS_FILE_1, FINAL_PKL_1, 'zh')
    m2, p2 = process_kg("KG2", ent_2, rel_2, trip_2, attr_2,
                        PROGRESS_FILE_2, FINAL_PKL_2, 'en')

    # ä¿å­˜æœºæ¢°ç‰ˆ (æ¶¦è‰²ç‰ˆåœ¨ process_kg é‡Œå·²ç»ä¿å­˜äº†)
    with open("data/demo_mini/zh_en/desc_mech_1.pkl", 'wb') as f:
        pickle.dump(m1, f)
    with open("data/demo_mini/zh_en/desc_mech_2.pkl", 'wb') as f:
        pickle.dump(m2, f)

    print("\nğŸ‰ å…¨é‡æ‰¹é‡æ¶¦è‰²ç»“æŸï¼")


if __name__ == "__main__":
    run()
