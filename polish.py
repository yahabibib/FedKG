# ğŸ“„ step2_local_llm.py
# ã€Prompt é€»è¾‘åŠ å›ºç‰ˆã€‘
# ä¿®å¤äº†"å¼ å† ææˆ´"é—®é¢˜ (é˜²æ­¢å°†é‚»å±…å±æ€§å®‰åˆ°å¤´å®ä½“ä¸Š)
# ç¼©çŸ­èƒŒæ™¯ä¿¡æ¯é•¿åº¦ï¼Œé˜²æ­¢å–§å®¾å¤ºä¸»

import os
import pickle
import torch
import config
import data_loader
from tqdm import tqdm
import re
import time
from transformers import AutoModelForCausalLM, AutoTokenizer

# ==========================================
# ğŸ”§ æ¨¡å‹é…ç½®
# ==========================================
MODEL_ID = "Qwen/Qwen2.5-1.5B-Instruct"
DEVICE = "cpu"

print(f"{'='*60}")
print(f"ğŸ¤– æ·±åº¦ç»“æ„åŒ–æ¶¦è‰² (Logic-Safe Prompt)")
print(f"   Model: {MODEL_ID}")
print(f"{'='*60}")

# åŠ è½½æ¨¡å‹
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID, torch_dtype=torch.float32, device_map=DEVICE, trust_remote_code=True)
except Exception as e:
    print(f"âŒ Load failed: {e}")
    exit()


def call_local_llm(prompt):
    messages = [{"role": "system", "content": "You are a helpful knowledge graph assistant."}, {
        "role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    with torch.no_grad():
        generated_ids = model.generate(
            model_inputs.input_ids,
            attention_mask=model_inputs.attention_mask,
            max_new_tokens=150,
            do_sample=False
        )
    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(
        model_inputs.input_ids, generated_ids)]
    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()


def clean_name(uri):
    if not isinstance(uri, str):
        return str(uri)
    name = uri.split('/')[-1].replace('_',
                                      ' ').replace('<', '').replace('>', '')
    name = re.sub(r'(?<!^)(?=[A-Z])', ' ', name).lower()
    return name.strip()

# --- æ™ºèƒ½æ‘˜è¦æå– (ç¼©çŸ­é•¿åº¦) ---


def get_smart_summary(text):
    if not text:
        return ""
    text = re.sub(r"\(.*?\)", "", text).replace("ï¼ˆ", "").replace("ï¼‰", "")
    if "ã€‚" in text:
        sentences = text.split("ã€‚")
    else:
        sentences = text.split(". ")
    summary = sentences[0].strip()
    # ç§»é™¤å¤ªé•¿çš„èƒŒæ™¯ï¼Œåªä¿ç•™å‰60ä¸ªå­—ç¬¦
    return summary[:60]


def run():
    demo_path = "data/demo_mini/zh_en/"
    if not os.path.exists(demo_path + "ent_ids_1"):
        print("âŒ æœªæ‰¾åˆ° Mini æ•°æ®é›†")
        return

    # 1. åŠ è½½æ•°æ®
    print("ğŸ“š åŠ è½½æ•°æ®...")
    ent_1, _ = data_loader.load_id_map(demo_path + "ent_ids_1")
    ent_2, _ = data_loader.load_id_map(demo_path + "ent_ids_2")
    rel_1, _ = data_loader.load_id_map(demo_path + "rel_ids_1")
    rel_2, _ = data_loader.load_id_map(demo_path + "rel_ids_2")
    trip_1 = data_loader.load_triples(demo_path + "triples_1")
    trip_2 = data_loader.load_triples(demo_path + "triples_2")

    with open(demo_path + "description1.pkl", 'rb') as f:
        attr_1 = pickle.load(f)
    with open(demo_path + "description2.pkl", 'rb') as f:
        attr_2 = pickle.load(f)

    # 2. å¤„ç†å‡½æ•°
    def process_dataset(ent_map, rel_map, triples, attr_map, lang_code):
        mech_dict = {}
        polish_dict = {}

        adj = {}
        for h, r, t in triples:
            if h not in adj:
                adj[h] = []
            adj[h].append((r, t))

        lang_name = "Chinese" if lang_code == 'zh' else "English"
        print(f"\nğŸš€ Processing {len(ent_map)} entities for {lang_name}...")

        stats = {"polished": 0, "skipped": 0}
        pbar = tqdm(ent_map.items())

        for i, (eid, uri) in enumerate(pbar):
            name = clean_name(uri)
            base_desc = attr_map.get(eid, "")

            neighbors = adj.get(eid, [])[:4]

            rich_triples = []
            simple_triples = []

            for r, t in neighbors:
                r_n = clean_name(rel_map.get(r, "rel"))
                t_n = clean_name(ent_map.get(t, "ent"))
                t_desc_raw = attr_map.get(t, "")
                t_ctx = get_smart_summary(t_desc_raw)

                if t_ctx:
                    rich_item = f"- å…³ç³»: {r_n} -> å¯¹è±¡: {t_n} (å¯¹è±¡èƒŒæ™¯: {t_ctx})"
                else:
                    rich_item = f"- å…³ç³»: {r_n} -> å¯¹è±¡: {t_n}"

                rich_triples.append(rich_item)
                simple_triples.append(f"{r_n}: {t_n}")

            if not simple_triples:
                mech_dict[eid] = base_desc
                polish_dict[eid] = base_desc
                stats["skipped"] += 1
                continue

            # A. æœºæ¢°ç‰ˆ
            mech_dict[eid] = f"{base_desc} [SEP] Structure: {'; '.join(simple_triples)}"

            # B. æ¶¦è‰²ç‰ˆ (Prompt é€»è¾‘åŠ å›º)
            data_str = "\n".join(rich_triples)

            if lang_code == 'zh':
                prompt = (
                    f"è¯·å°†ä»¥ä¸‹å…³äºä¸»è¯­â€œ{name}â€çš„çŸ¥è¯†å›¾è°±æ•°æ®ï¼Œæ”¹å†™æˆä¸€æ®µé€šé¡ºçš„ä¸­æ–‡ä»‹ç»ã€‚\n"
                    f"ã€æ•°æ®è¯´æ˜ã€‘\n"
                    f"æ ¼å¼ä¸ºï¼š'- å…³ç³»: X -> å¯¹è±¡: Y (å¯¹è±¡èƒŒæ™¯: Z)'\n"
                    f"æ³¨æ„ï¼šZ æ˜¯å¯¹å¯¹è±¡ Y çš„æè¿°ï¼Œ**ç»å¯¹ä¸æ˜¯**å¯¹ä¸»è¯­â€œ{name}â€çš„æè¿°ï¼ä¸è¦å¼ å† ææˆ´ã€‚\n\n"
                    f"ã€è¦æ±‚ã€‘\n"
                    f"1. å¿…é¡»ä»¥â€œ{name}â€å¼€å¤´ã€‚\n"
                    f"2. åŒ…å«æ‰€æœ‰å…³ç³»å’Œå¯¹è±¡ã€‚\n"
                    f"3. å¯ä»¥åˆ©ç”¨(å¯¹è±¡èƒŒæ™¯)ç®€å•è§£é‡Š Y æ˜¯ä»€ä¹ˆï¼Œä½†ä¸è¦ç…§æŠ„ï¼Œä¹Ÿä¸è¦æŠŠ Y çš„å±æ€§å®‰åœ¨â€œ{name}â€å¤´ä¸Šã€‚\n"
                    f"ã€æ•°æ®åˆ—è¡¨ã€‘\n{data_str}\n\n"
                    f"ç›´æ¥è¾“å‡ºç»“æœï¼š"
                )
            else:
                prompt = (
                    f"Summarize the KG data about '{name}' into a paragraph.\n"
                    f"ã€Formatã€‘\n"
                    f"'- Relation: X -> Object: Y (Context: Z)' means Z describes Y, NOT '{name}'.\n\n"
                    f"ã€Requirementsã€‘\n"
                    f"1. Start with '{name}'.\n"
                    f"2. Include all relations.\n"
                    f"3. Use (Context) to briefly explain Y, but DO NOT attribute Z's properties to '{name}'.\n\n"
                    f"ã€Dataã€‘\n{data_str}\n\n"
                    f"Output:"
                )

            polished = ""
            for _ in range(2):
                polished = call_local_llm(prompt)
                polished = polished.replace(
                    "Output:", "").replace("ç»“æœ:", "").strip()
                if len(polished) > 5:
                    break
                time.sleep(0.1)

            if not polished:
                polished = "; ".join(simple_triples)

            # ç›‘æ§
            if i % 20 == 0:
                tqdm.write("-" * 40)
                tqdm.write(f"ğŸ” [Monitor #{i}] Entity: {name}")
                tqdm.write(f"   In:\n{data_str}")
                tqdm.write(f"   Out:\n{polished}")
                tqdm.write("-" * 40)

            polish_dict[eid] = f"{base_desc} [SEP] {polished}"
            stats["polished"] += 1

        print(
            f"   Done. Polished: {stats['polished']}, Skipped: {stats['skipped']}")
        return mech_dict, polish_dict

    m1, p1 = process_dataset(ent_1, rel_1, trip_1, attr_1, 'zh')
    m2, p2 = process_dataset(ent_2, rel_2, trip_2, attr_2, 'en')

    print(f"\nğŸ’¾ ä¿å­˜ç»“æœ...")
    with open(demo_path + "desc_mech_1.pkl", 'wb') as f:
        pickle.dump(m1, f)
    with open(demo_path + "desc_mech_2.pkl", 'wb') as f:
        pickle.dump(m2, f)
    with open(demo_path + "desc_polish_1.pkl", 'wb') as f:
        pickle.dump(p1, f)
    with open(demo_path + "desc_polish_2.pkl", 'wb') as f:
        pickle.dump(p2, f)

    print("âœ… æ·±åº¦æ¶¦è‰²å®Œæˆï¼è¯·è¿è¡Œ step3_train_eval.py")


if __name__ == "__main__":
    run()
