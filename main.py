# main.py
import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import os
import torch
import torch.nn.functional as F
import json
import numpy as np

# --- å¯¼å…¥ç»„ä»¶ ---
from src.data.dataset import AlignmentTaskData
from src.utils.device_manager import DeviceManager
from src.utils.metrics import eval_alignment
from src.utils.logger import log_experiment_result

# --- è”é‚¦ç»„ä»¶ ---
from src.federation.server import Server
from src.federation.client_sbert import ClientSBERT
from src.federation.client_structure import ClientStructure
from src.federation.strategy import PseudoLabelGenerator

log = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•°ï¼šé˜ˆå€¼è®¡ç®— (Curriculum Learning) ---


def get_dynamic_threshold(start_threshold, end_threshold, total_rounds, current_round):
    """
    è®¡ç®—å½“å‰è½®æ¬¡çš„ä¼ªæ ‡ç­¾é˜ˆå€¼ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (é€æ­¥æé«˜é˜ˆå€¼)ã€‚
    """
    if current_round >= total_rounds:
        return end_threshold

    increment = (end_threshold - start_threshold) / total_rounds
    return start_threshold + current_round * increment


def to_dict(ids, embs):
    """å°†æœ‰åºçš„idså’Œembeddingsåˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸"""
    return {id: embs[i] for i, id in enumerate(ids)}

# -----------------------------------------------


@hydra.main(config_path="configs", config_name="config", version_base="1.2")
def main(cfg: DictConfig):
    """
    FedAnchor 2.0 ä¸»å…¥å£ (æ”¯æŒ SBERTå¾®è°ƒ å’Œ ç»“æ„è®­ç»ƒ åŒé˜¶æ®µ)
    """
    log.info(f"ğŸš€ Starting Experiment: {cfg.experiment_name}")
    log.info(
        f"âš™ï¸  Task Type: {cfg.task.type} | Mode: {cfg.task.strategy.text_mode}")

    dm = DeviceManager(cfg.system)

    log.info("ğŸ“š Loading Datasets...")
    try:
        task_data = AlignmentTaskData(cfg.data)
    except Exception as e:
        log.exception(f"âŒ Data loading failed: {e}")
        return

    server = Server(cfg)

    # 4. ä»»åŠ¡åˆ†å‘ (Task Dispatch)
    if cfg.task.type == 'sbert':
        log.info("ğŸ”¹ Entering Phase 1: SBERT Fine-tuning")
        c1 = ClientSBERT("C1", cfg, task_data.source, dm)
        c2 = ClientSBERT("C2", cfg, task_data.target, dm)
        run_sbert_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    elif cfg.task.type == 'structure':
        log.info("ğŸ”¹ Entering Phase 2: Structure Training (GAT Optimized)")
        log.info("ğŸ—ï¸ Initializing Structure Clients (Loading Frozen SBERT)...")
        c1 = ClientStructure("C1", cfg, task_data.source, dm)
        c2 = ClientStructure("C2", cfg, task_data.target, dm)
        run_structure_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    else:
        log.error(f"âŒ Unknown task type: {cfg.task.type}")


def run_sbert_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 1: SBERT æ··åˆå¾®è°ƒå·¥ä½œæµ (ä¿æŒåŸæ ·)
    """
    results = []
    rounds = cfg.task.federated.rounds
    base_threshold = cfg.task.strategy.pseudo_threshold
    text_mode = cfg.task.strategy.text_mode

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.75 -> 0.85
    threshold_start = 0.75

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ”„ [SBERT] Round {r}/{rounds} [{text_mode}] (Thresh: {current_threshold:.4f})\n{'='*40}")

        # 1. Encode
        ids1_desc, emb1_desc = c1.encode('desc')
        ids2_desc, emb2_desc = c2.encode('desc')
        ids1_poly, emb1_poly = c1.encode('polish')
        ids2_poly, emb2_poly = c2.encode('polish')

        # 2. Evaluate
        d1_desc = to_dict(ids1_desc, emb1_desc)
        d2_desc = to_dict(ids2_desc, emb2_desc)
        h_d, m_d = eval_alignment(d1_desc, d2_desc, test_pairs, device='cpu')

        d1_poly = to_dict(ids1_poly, emb1_poly)
        d2_poly = to_dict(ids2_poly, emb2_poly)
        h_p, m_p = eval_alignment(d1_poly, d2_poly, test_pairs, device='cpu')

        log.info(
            f"   ğŸ† Result R{r}: Desc H@1={h_d[1]:.2f}% | Polish H@1={h_p[1]:.2f}%")
        results.append(
            {"round": r, "desc_hits1": h_d[1], "desc_mrr": m_d, "poly_hits1": h_p[1], "poly_mrr": m_p})

        if r == rounds:
            break

        # 3. Strategy (Pseudo Labels)
        log.info(
            f"   Generating Pseudo-labels (Threshold={current_threshold:.4f})...")
        pairs_idx = PseudoLabelGenerator.generate(
            emb1_desc, emb2_desc, current_threshold, device='cpu')
        log.info(f"   ğŸŒ± Found {len(pairs_idx)} high-confidence pairs.")

        if len(pairs_idx) < 50:
            log.warning("   âš ï¸ Too few pairs, skipping training.")
            continue

        # 4. Prepare & Train
        p_idx1 = [p[0] for p in pairs_idx]
        p_idx2 = [p[1] for p in pairs_idx]
        target_desc_c1 = emb2_desc[p_idx2]
        target_desc_c2 = emb1_desc[p_idx1]
        target_poly_c1 = emb2_poly[p_idx2]
        target_poly_c2 = emb1_poly[p_idx1]

        c1.prepare_training_data(p_idx1, target_desc_c1, target_poly_c1)
        # Fixed minor variable name typo from previous version if any
        c2.prepare_training_data(p_idx2, target_desc_c2, target_poly_c2)

        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        c1.model.load_state_dict(server.global_model.state_dict())
        c2.model.load_state_dict(server.global_model.state_dict())
        dm.clean_memory()

    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"{text_mode}_round{rounds}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


def run_structure_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 2: Structure Alignment Workflow
    (Performance-Aware Curriculum Federation)
    """
    rounds = cfg.task.federated.rounds
    # è¯»å–è¯¾ç¨‹å­¦ä¹ é˜ˆå€¼ï¼Œé»˜è®¤ 0.70
    curriculum_thresh = cfg.task.federated.get('curriculum_thresh', 0.70)

    # æŒ–æ˜é˜ˆå€¼è¡°å‡ç­–ç•¥
    thresh_start = 0.85
    thresh_end = 0.60
    thresh_step = (thresh_start - thresh_end) / max(1, rounds - 1)

    best_hits1 = 0.0
    results_history = []

    # ---------------------------------------------------------
    # 0. åˆå§‹åŸºå‡†è¯„ä¼° (SBERT Baseline)
    # ---------------------------------------------------------
    log.info("\n" + "="*60)
    log.info("ğŸ“Š Baseline Evaluation: SBERT (Before Structure Training)")
    log.info("="*60)

    # è·å–çº¯ SBERT ç‰¹å¾
    s1_base = F.normalize(c1.anchor_embeddings, p=2, dim=1)
    s2_base = F.normalize(c2.anchor_embeddings, p=2, dim=1)

    d1_base = {id: s1_base[i] for i, id in enumerate(c1.dataset.ids)}
    d2_base = {id: s2_base[i] for i, id in enumerate(c2.dataset.ids)}

    # alpha=0.0 ä»£è¡¨çº¯ SBERT è¯„ä¼°
    h_base, mrr_base = eval_alignment(
        d1_base, d2_base, test_pairs, k_values=[1], alpha=0.0)
    log.info(f"ğŸ† SBERT Baseline: Hits@1={h_base[1]:.2f}% | MRR={mrr_base:.4f}")
    log.info("   (Target: Structure model should try to match this fidelity first!)\n")

    # ---------------------------------------------------------
    # è”é‚¦è®­ç»ƒå¾ªç¯
    # ---------------------------------------------------------
    for r in range(rounds + 1):
        # è®¡ç®—å½“å‰æŒ–æ˜é˜ˆå€¼
        curr_mining_thresh = max(thresh_end, thresh_start - (r * thresh_step))
        # åŠ¨æ€ Epochs: Round 0 éœ€è¦å¤šè·‘ä¸€ä¼šæ¥çƒ­èº«
        curr_epochs = 100 if r == 0 else cfg.task.federated.local_epochs

        log.info(f"\n{'='*40}")
        log.info(f"ğŸ—ï¸  [Structure] Round {r}/{rounds}")
        log.info(f"{'='*40}")

        # --- Step 1: Local Training ---
        log.info(
            f"ğŸš€ Training {cfg.task.model.encoder_name.upper()} (Target=SBERT/Peer)...")

        # æ¥æ”¶: æƒé‡, Loss, ä»¥åŠ [Internal Fidelity]
        w1, l1, fid1 = c1.train(custom_epochs=curr_epochs)
        w2, l2, fid2 = c2.train(custom_epochs=curr_epochs)

        avg_fidelity = (fid1 + fid2) / 2
        log.info(f"   ğŸ“‰ Loss: C1={l1:.4f} | C2={l2:.4f}")
        log.info(
            f"   ğŸ“ Internal Fidelity: C1={fid1:.3f} | C2={fid2:.3f} | Avg={avg_fidelity:.3f}")
        log.info(f"      (Curriculum Threshold: {curriculum_thresh})")

        # --- Step 2: Aggregation ---
        # log.info("ğŸ”— Aggregating Shared Weights...")
        server.aggregate([w1, w2])
        global_weights = server.get_global_weights()

        # åˆ†å‘æ›´æ–°
        c1.model.load_shared_state_dict(global_weights)
        c2.model.load_shared_state_dict(global_weights)

        # --- Step 3: Dual Evaluation (åŒé‡è¯„ä¼°) ---
        log.info(f"ğŸ“Š Round {r} Evaluation...")

        c1.model.to(c1.device).eval()
        c2.model.to(c2.device).eval()

        with torch.no_grad():
            # A. è·å–çº¯ç»“æ„ç‰¹å¾ (Pure Structure)
            emb1_struct = F.normalize(
                c1.model(c1.adj, c1.edge_types), p=2, dim=1)
            emb2_struct = F.normalize(
                c2.model(c2.adj, c2.edge_types), p=2, dim=1)

            # B. è·å– SBERT ç‰¹å¾ (Anchors)
            emb1_sbert = F.normalize(
                c1.anchor_embeddings.to(c1.device), p=2, dim=1)
            emb2_sbert = F.normalize(
                c2.anchor_embeddings.to(c2.device), p=2, dim=1)

            # C. èåˆç‰¹å¾ (Gate è¾…åŠ©æ¨ç†)
            # åªæœ‰åœ¨æ¨ç†æ—¶æ‰è¿›è¡Œèåˆï¼
            emb1_fused, alpha1 = c1.model.fuse(emb1_struct, emb1_sbert)
            emb2_fused, alpha2 = c2.model.fuse(emb2_struct, emb2_sbert)

            # æ‰“å° Gate çš„å€¾å‘æ€§
            log.info(
                f"      [Gate Stats] C1_Struct_Weight: {alpha1.mean():.3f} | C2_Struct_Weight: {alpha2.mean():.3f}")

            # å‡†å¤‡å­—å…¸ç”¨äº eval_alignment
            d1_s = {id: emb1_struct[i].cpu()
                    for i, id in enumerate(c1.dataset.ids)}
            d2_s = {id: emb2_struct[i].cpu()
                    for i, id in enumerate(c2.dataset.ids)}

            d1_f = {id: emb1_fused[i].cpu()
                    for i, id in enumerate(c1.dataset.ids)}
            d2_f = {id: emb2_fused[i].cpu()
                    for i, id in enumerate(c2.dataset.ids)}

        # æ¸…ç†æ˜¾å­˜
        c1.model.to('cpu')
        c2.model.to('cpu')
        if dm.is_offload_enabled():
            dm.clean_memory()

        # 3.1 è¯„ä¼°çº¯ç»“æ„ (Student Grade) -> çœ‹ GAT å­¦å¾—æ€ä¹ˆæ ·
        h_s, mrr_s = eval_alignment(
            d1_s, d2_s, test_pairs, k_values=[1], alpha=1.0)

        # 3.2 è¯„ä¼°èåˆæ•ˆæœ (Final Grade) -> å®é™…éƒ¨ç½²æ•ˆæœ
        h_f, mrr_f = eval_alignment(
            d1_f, d2_f, test_pairs, k_values=[1, 10], alpha=1.0)

        log.info(
            f"   ğŸ”¹ [Pure Structure] Hits@1={h_s[1]:.2f}% | MRR={mrr_s:.4f}")
        log.info(
            f"   ğŸ† [Fused Model   ] Hits@1={h_f[1]:.2f}% | Hits@10={h_f[10]:.2f}%")

        # è®°å½•ç»“æœ
        results_history.append({
            "round": r,
            "fidelity": avg_fidelity,
            "pure_hits1": h_s[1],
            "fused_hits1": h_f[1],
            "fused_hits10": h_f[10]
        })

        if h_f[1] > best_hits1:
            best_hits1 = h_f[1]
            server.save_model("best")

        if r == rounds:
            break

        # --- Step 4: Curriculum-Controlled Mining (è¯¾ç¨‹å­¦ä¹ æ§åˆ¶) ---
        # æ ¸å¿ƒé€»è¾‘ï¼šåªæœ‰å½“ Fidelity > Thresh æ—¶ï¼Œæ‰è®¤ä¸º Structure æ¨¡å‹â€œæ‡‚äº†â€ï¼Œå…è®¸å®ƒå»æŒ–æ˜
        if avg_fidelity < curriculum_thresh:
            log.warning(
                f"   âš ï¸ Fidelity ({avg_fidelity:.3f}) < Thresh ({curriculum_thresh}). Skipping Mining.")
            log.info("      (Student is not ready yet. Continuing Imitation...)")
            continue

        log.info(
            f"   ğŸ’ Fidelity Passed! Generating Pseudo-labels (Thresh={curr_mining_thresh:.4f})...")

        # ä½¿ç”¨ã€èåˆç‰¹å¾ã€‘è¿›è¡ŒæŒ–æ˜ï¼Œå› ä¸ºå®ƒæ˜¯æœ€å¼ºçš„
        new_pairs = PseudoLabelGenerator.generate(
            emb1_fused.cpu(), emb2_fused.cpu(),
            threshold=curr_mining_thresh,
            device='cpu'
        )

        if len(new_pairs) > 0:
            # äº’å­¦ä¹ ï¼šå¦‚æœ Structure è®¤ä¸º A-B å¯¹é½ï¼Œåˆ™æ›´æ–° SBERT Anchor
            # è¿™æ ·ä¸‹ä¸€è½® GAT å°±ä¼šè¢«å¼ºè¿«å»æ‹Ÿåˆè¿™ä¸ªæ–°çš„ã€å¸¦æœ‰ç»“æ„ä¿¡æ¯çš„ç›®æ ‡
            src_idx = [p[0] for p in new_pairs]
            tgt_idx = [p[1] for p in new_pairs]

            # C1 çš„æ–°ç›®æ ‡æ˜¯ C2 çš„ Embedding
            c1.update_anchors(src_idx, emb2_fused[tgt_idx].cpu())
            # C2 çš„æ–°ç›®æ ‡æ˜¯ C1 çš„ Embedding
            c2.update_anchors(tgt_idx, emb1_fused[src_idx].cpu())

            log.info(f"   âœ… Anchors Updated: {len(new_pairs)} pairs injected.")
        else:
            log.info("   âš ï¸ No reliable pairs found.")

    log.info(f"ğŸ Final Best Hits@1: {best_hits1:.2f}%")

    # ä¿å­˜è®­ç»ƒå†å²
    history_path = os.path.join(os.getcwd(), "training_history.json")
    with open(history_path, 'w') as f:
        json.dump(results_history, f, indent=4)

    res = {
        "dataset": cfg.data.name,
        "mode": "structure",
        "best_hits1": best_hits1
    }
    log_experiment_result("structure_phase2", cfg.data.name, res)


if __name__ == "__main__":
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    main()
