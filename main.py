import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import os
import torch
import torch.nn.functional as F

# --- å¯¼å…¥ç»„ä»¶ ---
from src.data.dataset import AlignmentTaskData
from src.utils.device_manager import DeviceManager
from src.utils.metrics import eval_alignment
from src.utils.logger import log_experiment_result

# --- è”é‚¦ç»„ä»¶ ---
from src.federation.server import Server
from src.federation.client_sbert import ClientSBERT
from src.federation.client_structure import ClientStructure
from src.federation.strategy import PseudoLabelGenerator

log = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•°ï¼šé˜ˆå€¼è®¡ç®— (Curriculum Learning) ---


def get_dynamic_threshold(start_threshold, end_threshold, total_rounds, current_round):
    """
    è®¡ç®—å½“å‰è½®æ¬¡çš„ä¼ªæ ‡ç­¾é˜ˆå€¼ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (é€æ­¥æé«˜é˜ˆå€¼)ã€‚
    """
    if current_round >= total_rounds:
        return end_threshold

    increment = (end_threshold - start_threshold) / total_rounds
    return start_threshold + current_round * increment


def _fuse_embeddings(struct_dict, sbert_dict, alpha):
    """
    è¾…åŠ©å‡½æ•°ï¼šåŠ æƒèåˆä¸¤ä¸ª Embedding å­—å…¸
    Res = alpha * Struct + (1-alpha) * SBERT
    """
    fused = {}
    # ç¡®ä¿IDå­˜åœ¨ä¸”æœ‰åº
    for eid, s_emb in struct_dict.items():
        if eid in sbert_dict:
            # å½’ä¸€åŒ– (éå¸¸é‡è¦)
            v1 = F.normalize(s_emb, dim=0)
            v2 = F.normalize(sbert_dict[eid], dim=0)
            fused[eid] = alpha * v1 + (1.0 - alpha) * v2
    return fused

# --- è¾…åŠ©å‡½æ•°ï¼šå­—å…¸è½¬æ¢ ---


def to_dict(ids, embs):
    """å°†æœ‰åºçš„idså’Œembeddingsåˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸"""
    return {id: embs[i] for i, id in enumerate(ids)}
# -----------------------------------------------


@hydra.main(config_path="configs", config_name="config", version_base="1.2")
def main(cfg: DictConfig):
    """
    FedAnchor 2.0 ä¸»å…¥å£ (æ”¯æŒ SBERTå¾®è°ƒ å’Œ ç»“æ„è®­ç»ƒ åŒé˜¶æ®µ)
    """
    log.info(f"ğŸš€ Starting Experiment: {cfg.experiment_name}")
    log.info(
        f"âš™ï¸  Task Type: {cfg.task.type} | Mode: {cfg.task.strategy.text_mode}")

    dm = DeviceManager(cfg.system)

    log.info("ğŸ“š Loading Datasets...")
    try:
        task_data = AlignmentTaskData(cfg.data)
    except Exception as e:
        log.exception(f"âŒ Data loading failed: {e}")
        return

    server = Server(cfg)

    # 4. ä»»åŠ¡åˆ†å‘ (Task Dispatch)
    if cfg.task.type == 'sbert':
        log.info("ğŸ”¹ Entering Phase 1: SBERT Fine-tuning")
        c1 = ClientSBERT("C1", cfg, task_data.source, dm)
        c2 = ClientSBERT("C2", cfg, task_data.target, dm)
        run_sbert_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    elif cfg.task.type == 'structure':
        log.info("ğŸ”¹ Entering Phase 2: Structure Training (GCN)")
        log.info("ğŸ—ï¸ Initializing Structure Clients (Loading Frozen SBERT)...")
        c1 = ClientStructure("C1", cfg, task_data.source, dm)
        c2 = ClientStructure("C2", cfg, task_data.target, dm)
        run_structure_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    else:
        log.error(f"âŒ Unknown task type: {cfg.task.type}")


def run_sbert_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 1: SBERT æ··åˆå¾®è°ƒå·¥ä½œæµ (å«è¯¾ç¨‹å­¦ä¹ )
    """
    results = []
    rounds = cfg.task.federated.rounds
    base_threshold = cfg.task.strategy.pseudo_threshold
    text_mode = cfg.task.strategy.text_mode

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.75 -> 0.85
    threshold_start = 0.75

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ”„ [SBERT] Round {r}/{rounds} [{text_mode}] (Thresh: {current_threshold:.4f})\n{'='*40}")

        # 1. Encode
        ids1_desc, emb1_desc = c1.encode('desc')
        ids2_desc, emb2_desc = c2.encode('desc')
        ids1_poly, emb1_poly = c1.encode('polish')
        ids2_poly, emb2_poly = c2.encode('polish')

        # 2. Evaluate
        d1_desc = to_dict(ids1_desc, emb1_desc)
        d2_desc = to_dict(ids2_desc, emb2_desc)
        h_d, m_d = eval_alignment(d1_desc, d2_desc, test_pairs, device='cpu')
        d1_poly = to_dict(ids1_poly, emb1_poly)
        d2_poly = to_dict(ids2_poly, emb2_poly)
        h_p, m_p = eval_alignment(d1_poly, d2_poly, test_pairs, device='cpu')

        log.info(
            f"   ğŸ† Result R{r}: Desc H@1={h_d[1]:.2f}% | Polish H@1={h_p[1]:.2f}%")
        results.append(
            {"round": r, "desc_hits1": h_d[1], "desc_mrr": m_d, "poly_hits1": h_p[1], "poly_mrr": m_p})

        if r == rounds:
            break

        # 3. Strategy (Pseudo Labels)
        log.info(
            f"   Generating Pseudo-labels (Threshold={current_threshold:.4f})...")
        pairs_idx = PseudoLabelGenerator.generate(
            emb1_desc, emb2_desc, current_threshold, device='cpu')
        log.info(f"   ğŸŒ± Found {len(pairs_idx)} high-confidence pairs.")

        if len(pairs_idx) < 50:
            log.warning("   âš ï¸ Too few pairs, skipping training.")
            continue

        # 4. Prepare & Train
        p_idx1 = [p[0] for p in pairs_idx]
        p_idx2 = [p[1] for p in pairs_idx]
        target_desc_c1 = emb2_desc[p_idx2]
        target_desc_c2 = emb1_desc[p_idx1]
        target_poly_c1 = emb2_poly[p_idx2]
        target_poly_c2 = emb1_poly[p_idx1]

        c1.prepare_training_data(p_idx1, target_desc_c1, target_poly_c1)
        c2.prepare_training_data(p_idx2, target_desc_c2, target_poly_for_c2)

        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        c1.model.load_state_dict(server.global_model.state_dict())
        c2.model.load_state_dict(server.global_model.state_dict())
        dm.clean_memory()

    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"{text_mode}_round{rounds}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


def run_structure_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 2: ç»“æ„è®­ç»ƒå·¥ä½œæµ (GCN Training) - å«èåˆä¼ªæ ‡ç­¾å’Œè¯¾ç¨‹å­¦ä¹ 
    """
    results = []
    rounds = cfg.task.federated.rounds
    alpha = cfg.task.eval.alpha
    base_threshold = cfg.task.strategy.pseudo_threshold  # 0.95

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.70 -> 0.95 (é™ä½èµ·ç‚¹ï¼Œç¡®ä¿ Round 1 èƒ½æ‰¾åˆ°ä¼ªæ ‡ç­¾)
    threshold_start = 0.70  # <--- å…³é”®ä¿®å¤ï¼šé™ä½èµ·å§‹é˜ˆå€¼

    # è·å– SBERT é”šç‚¹ (Primary Anchor Set)
    sb_emb1 = to_dict(c1.dataset.ids, c1.anchor_embeddings)
    sb_emb2 = to_dict(c2.dataset.ids, c2.anchor_embeddings)

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ—ï¸  [Structure] Round {r}/{rounds} (Thresh: {current_threshold:.4f})\n{'='*40}")

        # --- Step 1: æ¨ç† (Inference) ---
        struct_emb1 = c1.get_embeddings()
        struct_emb2 = c2.get_embeddings()

        st_dict1 = to_dict(c1.dataset.ids, struct_emb1)
        st_dict2 = to_dict(c2.dataset.ids, struct_emb2)

        # --- Step 2: èåˆè¯„ä¼° (Fusion) ---
        log.info(f"   ğŸ“Š Eval [Fusion Alpha={alpha}]...")
        fused_1 = _fuse_embeddings(st_dict1, sb_emb1, alpha)
        fused_2 = _fuse_embeddings(st_dict2, sb_emb2, alpha)

        h_f, m_f = eval_alignment(fused_1, fused_2, test_pairs, device='cpu')

        log.info(f"   ğŸ† Result R{r}: Hits@1={h_f[1]:.2f}% | MRR={m_f:.4f}")
        results.append({"round": r, "hits1": h_f[1], "mrr": m_f})

        if r == rounds:
            break

        # --- Step 3: ç­–ç•¥ - GCN + SBERT èåˆé©±åŠ¨çš„ä¼ªæ ‡ç­¾ç”Ÿæˆ ---
        log.info(
            f"   ìœµ Generating Fusion-driven Pseudo-labels (Alpha={alpha}, Thresh={current_threshold:.4f})...")

        # æ ¸å¿ƒï¼šä½¿ç”¨èåˆç›¸ä¼¼åº¦å‘ç°æ–°çš„é«˜è´¨é‡é”šç‚¹
        fusion_pairs_idx = PseudoLabelGenerator.generate_fusion(
            struct_emb1, c1.anchor_embeddings,
            struct_emb2, c2.anchor_embeddings,
            alpha=alpha,
            threshold=current_threshold,
            device='cpu'
        )

        log.info(
            f"   Found {len(fusion_pairs_idx)} new pairs for anchor expansion.")

        if len(fusion_pairs_idx) < 50:
            log.warning(
                "   âš ï¸ Too few fusion-driven anchors, skipping training.")
            continue

        # --- Step 4: é”šç‚¹æ‰©å±•ä¸è®­ç»ƒ (Train) ---
        p_idx1 = [p[0] for p in fusion_pairs_idx]
        p_idx2 = [p[1] for p in fusion_pairs_idx]

        # ç›®æ ‡ä¿¡å·ï¼šä½¿ç”¨ GCN çš„è¾“å‡ºä½œä¸ºæ–°çš„ Teacher ä¿¡å· (åŠ¨æ€é”šç‚¹)
        new_anchors_for_c1 = struct_emb2[p_idx2]
        new_anchors_for_c2 = struct_emb1[p_idx1]

        # 1. æ‰©å±• Client çš„æœ¬åœ°é”šç‚¹é›†
        c1.update_anchors(p_idx1, new_anchors_for_c1)
        c2.update_anchors(p_idx2, new_anchors_for_c2)

        # 2. è®­ç»ƒ GCN (æœ¬åœ° Epochs=10)
        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        global_shared = server.get_global_weights()
        c1.model.load_shared_state_dict(global_shared)
        c2.model.load_shared_state_dict(global_shared)

        dm.clean_memory()

    # Save and Log (Final)
    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"structure_round{rounds}")
    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


if __name__ == "__main__":
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    main()
