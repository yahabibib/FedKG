import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import os
import torch
import torch.nn.functional as F

# --- å¯¼å…¥ç»„ä»¶ ---
from src.data.dataset import AlignmentTaskData
from src.utils.device_manager import DeviceManager
from src.utils.metrics import eval_alignment
from src.utils.logger import log_experiment_result

# --- è”é‚¦ç»„ä»¶ ---
from src.federation.server import Server
from src.federation.client_sbert import ClientSBERT
from src.federation.client_structure import ClientStructure
from src.federation.strategy import PseudoLabelGenerator

log = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•°ï¼šé˜ˆå€¼è®¡ç®— (Curriculum Learning) ---


def get_dynamic_threshold(start_threshold, end_threshold, total_rounds, current_round):
    """
    è®¡ç®—å½“å‰è½®æ¬¡çš„ä¼ªæ ‡ç­¾é˜ˆå€¼ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (é€æ­¥æé«˜é˜ˆå€¼)ã€‚
    """
    if current_round >= total_rounds:
        return end_threshold

    increment = (end_threshold - start_threshold) / total_rounds
    return start_threshold + current_round * increment


def _fuse_embeddings(struct_dict, sbert_dict, alpha):
    """
    è¾…åŠ©å‡½æ•°ï¼šåŠ æƒèåˆä¸¤ä¸ª Embedding å­—å…¸
    Res = alpha * Struct + (1-alpha) * SBERT
    """
    fused = {}
    # ç¡®ä¿IDå­˜åœ¨ä¸”æœ‰åº
    for eid, s_emb in struct_dict.items():
        if eid in sbert_dict:
            # å½’ä¸€åŒ– (éå¸¸é‡è¦)
            v1 = F.normalize(s_emb, dim=0)
            v2 = F.normalize(sbert_dict[eid], dim=0)
            fused[eid] = alpha * v1 + (1.0 - alpha) * v2
    return fused

# --- è¾…åŠ©å‡½æ•°ï¼šå­—å…¸è½¬æ¢ ---


def to_dict(ids, embs):
    """å°†æœ‰åºçš„idså’Œembeddingsåˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸"""
    return {id: embs[i] for i, id in enumerate(ids)}
# -----------------------------------------------


@hydra.main(config_path="configs", config_name="config", version_base="1.2")
def main(cfg: DictConfig):
    """
    FedAnchor 2.0 ä¸»å…¥å£ (æ”¯æŒ SBERTå¾®è°ƒ å’Œ ç»“æ„è®­ç»ƒ åŒé˜¶æ®µ)
    """
    log.info(f"ğŸš€ Starting Experiment: {cfg.experiment_name}")
    log.info(
        f"âš™ï¸  Task Type: {cfg.task.type} | Mode: {cfg.task.strategy.text_mode}")

    dm = DeviceManager(cfg.system)

    log.info("ğŸ“š Loading Datasets...")
    try:
        task_data = AlignmentTaskData(cfg.data)
    except Exception as e:
        log.exception(f"âŒ Data loading failed: {e}")
        return

    server = Server(cfg)

    # 4. ä»»åŠ¡åˆ†å‘ (Task Dispatch)
    if cfg.task.type == 'sbert':
        log.info("ğŸ”¹ Entering Phase 1: SBERT Fine-tuning")
        c1 = ClientSBERT("C1", cfg, task_data.source, dm)
        c2 = ClientSBERT("C2", cfg, task_data.target, dm)
        run_sbert_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    elif cfg.task.type == 'structure':
        log.info("ğŸ”¹ Entering Phase 2: Structure Training (GCN)")
        log.info("ğŸ—ï¸ Initializing Structure Clients (Loading Frozen SBERT)...")
        c1 = ClientStructure("C1", cfg, task_data.source, dm)
        c2 = ClientStructure("C2", cfg, task_data.target, dm)
        run_structure_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    else:
        log.error(f"âŒ Unknown task type: {cfg.task.type}")


def run_sbert_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 1: SBERT æ··åˆå¾®è°ƒå·¥ä½œæµ (å«è¯¾ç¨‹å­¦ä¹ )
    """
    results = []
    rounds = cfg.task.federated.rounds
    base_threshold = cfg.task.strategy.pseudo_threshold
    text_mode = cfg.task.strategy.text_mode

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.75 -> 0.85
    threshold_start = 0.75

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ”„ [SBERT] Round {r}/{rounds} [{text_mode}] (Thresh: {current_threshold:.4f})\n{'='*40}")

        # 1. Encode
        ids1_desc, emb1_desc = c1.encode('desc')
        ids2_desc, emb2_desc = c2.encode('desc')
        ids1_poly, emb1_poly = c1.encode('polish')
        ids2_poly, emb2_poly = c2.encode('polish')

        # 2. Evaluate
        d1_desc = to_dict(ids1_desc, emb1_desc)
        d2_desc = to_dict(ids2_desc, emb2_desc)
        h_d, m_d = eval_alignment(d1_desc, d2_desc, test_pairs, device='cpu')
        d1_poly = to_dict(ids1_poly, emb1_poly)
        d2_poly = to_dict(ids2_poly, emb2_poly)
        h_p, m_p = eval_alignment(d1_poly, d2_poly, test_pairs, device='cpu')

        log.info(
            f"   ğŸ† Result R{r}: Desc H@1={h_d[1]:.2f}% | Polish H@1={h_p[1]:.2f}%")
        results.append(
            {"round": r, "desc_hits1": h_d[1], "desc_mrr": m_d, "poly_hits1": h_p[1], "poly_mrr": m_p})

        if r == rounds:
            break

        # 3. Strategy (Pseudo Labels)
        log.info(
            f"   Generating Pseudo-labels (Threshold={current_threshold:.4f})...")
        pairs_idx = PseudoLabelGenerator.generate(
            emb1_desc, emb2_desc, current_threshold, device='cpu')
        log.info(f"   ğŸŒ± Found {len(pairs_idx)} high-confidence pairs.")

        if len(pairs_idx) < 50:
            log.warning("   âš ï¸ Too few pairs, skipping training.")
            continue

        # 4. Prepare & Train
        p_idx1 = [p[0] for p in pairs_idx]
        p_idx2 = [p[1] for p in pairs_idx]
        target_desc_c1 = emb2_desc[p_idx2]
        target_desc_c2 = emb1_desc[p_idx1]
        target_poly_c1 = emb2_poly[p_idx2]
        target_poly_c2 = emb1_poly[p_idx1]

        c1.prepare_training_data(p_idx1, target_desc_c1, target_poly_c1)
        c2.prepare_training_data(p_idx2, target_desc_c2, target_poly_for_c2)

        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        c1.model.load_state_dict(server.global_model.state_dict())
        c2.model.load_state_dict(server.global_model.state_dict())
        dm.clean_memory()

    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"{text_mode}_round{rounds}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


def run_structure_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 2: ç»“æ„è®­ç»ƒå·¥ä½œæµ (GCN Training) - ä¿®å¤ç‰ˆ
    é€»è¾‘é¡ºåº: Train(åŸºäºç°æœ‰é”šç‚¹) -> Aggregate -> Eval -> Mine(ä¸ºä¸‹ä¸€è½®å‡†å¤‡)
    """
    results = []
    rounds = cfg.task.federated.rounds
    alpha = cfg.task.eval.alpha
    base_threshold = cfg.task.strategy.pseudo_threshold

    # é™ä½èµ·å§‹é˜ˆå€¼ï¼Œç»™ç¬¬ä¸€è½®æ›´å¤šæœºä¼š
    threshold_start = 0.70

    # è·å–åˆå§‹ SBERT é”šç‚¹ç”¨äºèåˆè®¡ç®—
    sb_emb1 = to_dict(c1.dataset.ids, c1.anchor_embeddings)
    sb_emb2 = to_dict(c2.dataset.ids, c2.anchor_embeddings)

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)

        log.info(
            f"\n{'='*40}\nğŸ—ï¸  [Structure] Round {r}/{rounds} (Thresh: {current_threshold:.4f})\n{'='*40}")

        # =================================================
        # 1. è®­ç»ƒ (Train) & èšåˆ (Aggregate)
        # =================================================
        # [æ ¸å¿ƒä¿®å¤] æ— è®ºæ˜¯å¦æŒ–æ˜åˆ°æ–°æ ‡ç­¾ï¼Œéƒ½å¿…é¡»åŸºäºç°æœ‰é”šç‚¹è®­ç»ƒ GCN
        # Round 0 æ—¶ï¼Œè¿™é‡Œä¼šåˆ©ç”¨åˆå§‹çš„ SBERT é”šç‚¹æŠŠéšæœº GCN è®­ç»ƒæˆæœ‰æ„ä¹‰çš„å½¢çŠ¶
        log.info("   ğŸš€ Training GCN on current anchors...")
        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # èšåˆæ›´æ–°
        server.aggregate([w1, w2])
        global_shared = server.get_global_weights()
        c1.model.load_shared_state_dict(global_shared)
        c2.model.load_shared_state_dict(global_shared)

        # æ¸…ç†æ˜¾å­˜
        dm.clean_memory()

        # =================================================
        # 2. è¯„ä¼° (Eval)
        # =================================================
        # è·å–è®­ç»ƒåçš„ Embeddings
        struct_emb1 = c1.get_embeddings()
        struct_emb2 = c2.get_embeddings()

        st_dict1 = to_dict(c1.dataset.ids, struct_emb1)
        st_dict2 = to_dict(c2.dataset.ids, struct_emb2)

        log.info(f"   ğŸ“Š Eval [Fusion Alpha={alpha}]...")
        fused_1 = _fuse_embeddings(st_dict1, sb_emb1, alpha)
        fused_2 = _fuse_embeddings(st_dict2, sb_emb2, alpha)

        h_f, m_f = eval_alignment(fused_1, fused_2, test_pairs, device='cpu')

        # è®°å½•ç»“æœ
        log.info(f"   ğŸ† Result R{r}: Hits@1={h_f[1]:.2f}% | MRR={m_f:.4f}")
        results.append({"round": r, "hits1": h_f[1], "mrr": m_f})

        if r == rounds:
            break

        # =================================================
        # 3. ç­–ç•¥æŒ–æ˜ (Mine & Update) - ä¸ºä¸‹ä¸€è½®åšå‡†å¤‡
        # =================================================
        log.info(
            f"   ìœµ Generating Fusion-driven Pseudo-labels (Alpha={alpha}, Thresh={current_threshold:.4f})...")

        # ä½¿ç”¨è®­ç»ƒåçš„ Struct Embedding è¿›è¡ŒæŒ–æ˜
        fusion_pairs_idx = PseudoLabelGenerator.generate_fusion(
            struct_emb1, c1.anchor_embeddings,
            struct_emb2, c2.anchor_embeddings,
            alpha=alpha,
            threshold=current_threshold,
            device='cpu'
        )

        log.info(
            f"   Found {len(fusion_pairs_idx)} new pairs for anchor expansion.")

        if len(fusion_pairs_idx) > 0:
            p_idx1 = [p[0] for p in fusion_pairs_idx]
            p_idx2 = [p[1] for p in fusion_pairs_idx]

            # æå–å¯¹ç«¯è®­ç»ƒå¥½çš„ Structure Embedding ä½œä¸ºæ–°çš„ Teacher ä¿¡å·
            # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ struct_emb (GCNè¾“å‡º) è¿˜æ˜¯ fused (èåˆ) å–å†³äºç­–ç•¥
            # åŸè®ºæ–‡é€šå¸¸æ˜¯ç”¨ Structure Embedding åšäº’ç›‘ç£
            new_anchors_for_c1 = struct_emb2[p_idx2]
            new_anchors_for_c2 = struct_emb1[p_idx1]

            # æ›´æ–° Client çš„æœ¬åœ°é”šç‚¹é›† (è¿™ä¼šå½±å“ä¸‹ä¸€è½®çš„ Train)
            c1.update_anchors(p_idx1, new_anchors_for_c1)
            c2.update_anchors(p_idx2, new_anchors_for_c2)
        else:
            log.info(
                "   âš ï¸ No new anchors found, keeping previous anchors for next round.")

    # Save and Log (Final)
    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"structure_round{rounds}")
    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


if __name__ == "__main__":
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    main()
