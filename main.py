import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import os
import torch
import torch.nn.functional as F

# --- å¯¼å…¥ç»„ä»¶ ---
from src.data.dataset import AlignmentTaskData
from src.utils.device_manager import DeviceManager
from src.utils.metrics import eval_alignment
from src.utils.logger import log_experiment_result

# --- è”é‚¦ç»„ä»¶ ---
from src.federation.server import Server
from src.federation.client_sbert import ClientSBERT
from src.federation.client_structure import ClientStructure
from src.federation.strategy import PseudoLabelGenerator

log = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•°ï¼šé˜ˆå€¼è®¡ç®— (Curriculum Learning) ---


def get_dynamic_threshold(start_threshold, end_threshold, total_rounds, current_round):
    """
    è®¡ç®—å½“å‰è½®æ¬¡çš„ä¼ªæ ‡ç­¾é˜ˆå€¼ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (é€æ­¥æé«˜é˜ˆå€¼)ã€‚
    """
    if current_round >= total_rounds:
        return end_threshold

    increment = (end_threshold - start_threshold) / total_rounds
    return start_threshold + current_round * increment


def _fuse_embeddings(struct_dict, sbert_dict, alpha):
    """
    è¾…åŠ©å‡½æ•°ï¼šåŠ æƒèåˆä¸¤ä¸ª Embedding å­—å…¸
    Res = alpha * Struct + (1-alpha) * SBERT
    """
    fused = {}
    # ç¡®ä¿IDå­˜åœ¨ä¸”æœ‰åº
    for eid, s_emb in struct_dict.items():
        if eid in sbert_dict:
            # å½’ä¸€åŒ– (éå¸¸é‡è¦)
            v1 = F.normalize(s_emb, dim=0)
            v2 = F.normalize(sbert_dict[eid], dim=0)
            fused[eid] = alpha * v1 + (1.0 - alpha) * v2
    return fused

# --- è¾…åŠ©å‡½æ•°ï¼šå­—å…¸è½¬æ¢ ---


def to_dict(ids, embs):
    """å°†æœ‰åºçš„idså’Œembeddingsåˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸"""
    return {id: embs[i] for i, id in enumerate(ids)}
# -----------------------------------------------


@hydra.main(config_path="configs", config_name="config", version_base="1.2")
def main(cfg: DictConfig):
    """
    FedAnchor 2.0 ä¸»å…¥å£ (æ”¯æŒ SBERTå¾®è°ƒ å’Œ ç»“æ„è®­ç»ƒ åŒé˜¶æ®µ)
    """
    log.info(f"ğŸš€ Starting Experiment: {cfg.experiment_name}")
    log.info(
        f"âš™ï¸  Task Type: {cfg.task.type} | Mode: {cfg.task.strategy.text_mode}")

    dm = DeviceManager(cfg.system)

    log.info("ğŸ“š Loading Datasets...")
    try:
        task_data = AlignmentTaskData(cfg.data)
    except Exception as e:
        log.exception(f"âŒ Data loading failed: {e}")
        return

    server = Server(cfg)

    # 4. ä»»åŠ¡åˆ†å‘ (Task Dispatch)
    if cfg.task.type == 'sbert':
        log.info("ğŸ”¹ Entering Phase 1: SBERT Fine-tuning")
        c1 = ClientSBERT("C1", cfg, task_data.source, dm)
        c2 = ClientSBERT("C2", cfg, task_data.target, dm)
        run_sbert_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    elif cfg.task.type == 'structure':
        log.info("ğŸ”¹ Entering Phase 2: Structure Training (GCN)")
        log.info("ğŸ—ï¸ Initializing Structure Clients (Loading Frozen SBERT)...")
        c1 = ClientStructure("C1", cfg, task_data.source, dm)
        c2 = ClientStructure("C2", cfg, task_data.target, dm)
        run_structure_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    else:
        log.error(f"âŒ Unknown task type: {cfg.task.type}")


def run_sbert_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 1: SBERT æ··åˆå¾®è°ƒå·¥ä½œæµ (å«è¯¾ç¨‹å­¦ä¹ )
    """
    results = []
    rounds = cfg.task.federated.rounds
    base_threshold = cfg.task.strategy.pseudo_threshold
    text_mode = cfg.task.strategy.text_mode

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.75 -> 0.85
    threshold_start = 0.75

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ”„ [SBERT] Round {r}/{rounds} [{text_mode}] (Thresh: {current_threshold:.4f})\n{'='*40}")

        # 1. Encode
        ids1_desc, emb1_desc = c1.encode('desc')
        ids2_desc, emb2_desc = c2.encode('desc')
        ids1_poly, emb1_poly = c1.encode('polish')
        ids2_poly, emb2_poly = c2.encode('polish')

        # 2. Evaluate
        d1_desc = to_dict(ids1_desc, emb1_desc)
        d2_desc = to_dict(ids2_desc, emb2_desc)
        h_d, m_d = eval_alignment(d1_desc, d2_desc, test_pairs, device='cpu')
        d1_poly = to_dict(ids1_poly, emb1_poly)
        d2_poly = to_dict(ids2_poly, emb2_poly)
        h_p, m_p = eval_alignment(d1_poly, d2_poly, test_pairs, device='cpu')

        log.info(
            f"   ğŸ† Result R{r}: Desc H@1={h_d[1]:.2f}% | Polish H@1={h_p[1]:.2f}%")
        results.append(
            {"round": r, "desc_hits1": h_d[1], "desc_mrr": m_d, "poly_hits1": h_p[1], "poly_mrr": m_p})

        if r == rounds:
            break

        # 3. Strategy (Pseudo Labels)
        log.info(
            f"   Generating Pseudo-labels (Threshold={current_threshold:.4f})...")
        pairs_idx = PseudoLabelGenerator.generate(
            emb1_desc, emb2_desc, current_threshold, device='cpu')
        log.info(f"   ğŸŒ± Found {len(pairs_idx)} high-confidence pairs.")

        if len(pairs_idx) < 50:
            log.warning("   âš ï¸ Too few pairs, skipping training.")
            continue

        # 4. Prepare & Train
        p_idx1 = [p[0] for p in pairs_idx]
        p_idx2 = [p[1] for p in pairs_idx]
        target_desc_c1 = emb2_desc[p_idx2]
        target_desc_c2 = emb1_desc[p_idx1]
        target_poly_c1 = emb2_poly[p_idx2]
        target_poly_c2 = emb1_poly[p_idx1]

        c1.prepare_training_data(p_idx1, target_desc_c1, target_poly_c1)
        c2.prepare_training_data(p_idx2, target_desc_c2, target_poly_for_c2)

        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        c1.model.load_state_dict(server.global_model.state_dict())
        c2.model.load_state_dict(server.global_model.state_dict())
        dm.clean_memory()

    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"{text_mode}_round{rounds}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


def run_structure_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 2: ç»“æ„è®­ç»ƒå·¥ä½œæµ (GCN Training) - å¤åˆ»è€ç‰ˆæœ¬äº’å­¦ä¹ ç­–ç•¥
    """
    results = []
    rounds = cfg.task.federated.rounds
    alpha = cfg.task.eval.alpha

    # [ç­–ç•¥è°ƒæ•´] é˜ˆå€¼ä»é«˜åˆ°ä½ (0.8 -> 0.5)
    # æ¨¡æ‹Ÿè€ç‰ˆæœ¬çš„: max(0.50, 0.80 - (it * 0.05))
    thresh_start = 0.80
    thresh_end = 0.50
    thresh_step = (thresh_start - thresh_end) / max(1, rounds - 1)

    # 1. å¤‡ä»½åŸå§‹ SBERT (ä»…ç”¨äºè¯„ä¼°å’Œè¯­ä¹‰è¿‡æ»¤å™¨ï¼Œä¸ä½œä¸ºè®­ç»ƒå¼ºçº¦æŸ)
    fixed_sbert_1 = c1.anchor_embeddings.clone().cpu()
    fixed_sbert_2 = c2.anchor_embeddings.clone().cpu()

    # è¯„ä¼°ç”¨å­—å…¸
    sb_emb1 = to_dict(c1.dataset.ids, fixed_sbert_1)
    sb_emb2 = to_dict(c2.dataset.ids, fixed_sbert_2)

    for r in range(rounds + 1):
        # è®¡ç®—å½“å‰é˜ˆå€¼ (Decaying)
        current_threshold = max(thresh_end, thresh_start - (r * thresh_step))

        # åŠ¨æ€ Epochs: Round 0 é“ºåº• (100)ï¼Œåç»­å¾®è°ƒ (20)
        current_epochs = cfg.task.federated.local_epochs if r == 0 else 20

        log.info(
            f"\n{'='*40}\nğŸ—ï¸  [Structure] Round {r}/{rounds} (Thresh: {current_threshold:.4f} | Epochs: {current_epochs})\n{'='*40}")

        # --- Step 1: è®­ç»ƒ (Train) ---
        log.info(f"   ğŸš€ Training GCN on current anchors (Mutual Targets)...")
        w1, l1 = c1.train(custom_epochs=current_epochs)
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train(custom_epochs=current_epochs)
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # --- Step 2: èšåˆ (Aggregate) ---
        server.aggregate([w1, w2])
        global_shared = server.get_global_weights()
        c1.model.load_shared_state_dict(global_shared)
        c2.model.load_shared_state_dict(global_shared)
        dm.clean_memory()

        # --- Step 3: è¯„ä¼° (Score Fusion) ---
        struct_emb1 = c1.get_embeddings()
        struct_emb2 = c2.get_embeddings()

        st_dict1 = to_dict(c1.dataset.ids, struct_emb1)
        st_dict2 = to_dict(c2.dataset.ids, struct_emb2)

        log.info(f"   ğŸ“Š Eval [Score Fusion Alpha={alpha}]...")
        # è¯„ä¼°å§‹ç»ˆä»¥ Fixed SBERT ä¸ºåŸºå‡†ï¼Œä¿æŒå…¬å¹³æ€§
        h_f, m_f = eval_alignment(
            st_dict1, st_dict2, test_pairs,
            sbert1_dict=sb_emb1, sbert2_dict=sb_emb2,
            alpha=alpha, device='cpu'
        )
        log.info(f"   ğŸ† Result R{r}: Hits@1={h_f[1]:.2f}% | MRR={m_f:.4f}")
        results.append({"round": r, "hits1": h_f[1], "mrr": m_f})

        if r == rounds:
            break

        # --- Step 4: æŒ–æ˜ (Pure GCN) ---
        log.info(
            f"   ğŸ’ Generating Pseudo-labels (GCN Mining, Decaying Thresh={current_threshold:.4f})...")

        pairs_idx = PseudoLabelGenerator.generate(
            struct_emb1, struct_emb2,
            threshold=current_threshold, device='cpu'
        )

        # --- Step 5: è¯­ä¹‰ä¸€è‡´æ€§è¿‡æ»¤ (Safety Net) ---
        # è™½ç„¶æˆ‘ä»¬æƒ³åšäº’å­¦ä¹ ï¼Œä½†ä¸ºäº†é˜²æ­¢å®Œå…¨è·‘åï¼ŒåŠ ä¸€ä¸ªå®½æ¾çš„ SBERT è¿‡æ»¤å™¨
        filtered_pairs = []
        semantic_filter_thresh = 0.25  # æ¯”è¾ƒå®½æ¾ï¼Œå…è®¸ä¸€å®šçš„è¯­ä¹‰å™ªéŸ³ï¼Œåªè¦ä¸å¤ªç¦»è°±

        if len(pairs_idx) > 0:
            p1 = torch.tensor([p[0] for p in pairs_idx])
            p2 = torch.tensor([p[1] for p in pairs_idx])

            s1_vecs = F.normalize(fixed_sbert_1[p1], p=2, dim=1)
            s2_vecs = F.normalize(fixed_sbert_2[p2], p=2, dim=1)
            sem_sims = (s1_vecs * s2_vecs).sum(dim=1)

            mask = sem_sims > semantic_filter_thresh
            valid_indices = torch.nonzero(mask).squeeze()

            if valid_indices.numel() > 0:
                if valid_indices.ndim == 0:
                    filtered_pairs.append(pairs_idx[valid_indices.item()])
                else:
                    for idx in valid_indices.tolist():
                        filtered_pairs.append(pairs_idx[idx])

            log.info(
                f"   ğŸ” Semantic Filter: {len(pairs_idx)} -> {len(filtered_pairs)} (Removed {len(pairs_idx)-len(filtered_pairs)})")
        else:
            filtered_pairs = []

        # --- Step 6: äº’å­¦ä¹ æ›´æ–° (Co-training Update) ---
        if len(filtered_pairs) > 0:
            p_idx1 = [p[0] for p in filtered_pairs]
            p_idx2 = [p[1] for p in filtered_pairs]

            # [è€ç‰ˆæœ¬é€»è¾‘å¤åˆ»]
            # C1 çš„æ–°ç›®æ ‡ = C2 ç°åœ¨çš„ Structure Embedding
            # è¿™å…è®¸ C1 å­¦ä¹  C2 æŒ–æ˜å‡ºçš„ç»“æ„ä¿¡æ¯ï¼Œè€Œä¸ä»…ä»…æ˜¯æ­»è®° SBERT
            new_anchors_for_c1 = struct_emb2[p_idx2]
            new_anchors_for_c2 = struct_emb1[p_idx1]

            c1.update_anchors(p_idx1, new_anchors_for_c1)
            c2.update_anchors(p_idx2, new_anchors_for_c2)

            log.info(
                f"   âœ… Anchors Expanded: +{len(filtered_pairs)} pairs (Targets = Peer Structure).")
        else:
            log.warning("   âš ï¸ No new anchors found.")

    if cfg.task.checkpoint.save_best:
        # è·å–å½“å‰ä½¿ç”¨çš„ encoder åç§° (gcn æˆ– gat)
        encoder_name = cfg.task.model.encoder_name

        # 1. ä¿å­˜ Server (Global MLP) -> åŠ ä¸Š encoder åç¼€
        server.save_model(suffix=f"structure_{encoder_name}_round{rounds}")

        # 2. ä¿å­˜ Client æ¨¡å‹ (åŒ…å«ç§æœ‰ GCN/GAT å‚æ•°)
        save_dir = cfg.task.checkpoint.save_dir
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        # å‘½åæ ¼å¼: c{id}_structure_round{rounds}.pth
        # [ä¿®æ”¹] å‘½åæ ¼å¼: c{id}_structure_{encoder}_round{rounds}.pth
        # è¿™æ · GCN å’Œ GAT çš„æƒé‡æ–‡ä»¶å°±ä¼šåˆ†å¼€ï¼Œä¸ä¼šè¦†ç›–
        c1_path = os.path.join(
            save_dir, f"c1_structure_{encoder_name}_round{rounds}.pth")
        c2_path = os.path.join(
            save_dir, f"c2_structure_{encoder_name}_round{rounds}.pth")

        # è·å–åŒ…å« GCN+MLP çš„å®Œæ•´çŠ¶æ€å­—å…¸
        # æ³¨æ„: get_shared_state_dict åªè¿”å› MLPï¼Œæˆ‘ä»¬è¦ç”¨ state_dict() è·å–å…¨éƒ¨
        torch.save(c1.model.state_dict(), c1_path)
        torch.save(c2.model.state_dict(), c2_path)

        log.info(f"ğŸ’¾ Saved full client models to:")
        log.info(f"   - {c1_path}")
        log.info(f"   - {c2_path}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


if __name__ == "__main__":
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    main()
