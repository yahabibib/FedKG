import hydra
from omegaconf import DictConfig, OmegaConf
import logging
import os
import torch
import torch.nn.functional as F

# --- å¯¼å…¥ç»„ä»¶ ---
from src.data.dataset import AlignmentTaskData
from src.utils.device_manager import DeviceManager
from src.utils.metrics import eval_alignment
from src.utils.logger import log_experiment_result

# --- è”é‚¦ç»„ä»¶ ---
from src.federation.server import Server
from src.federation.client_sbert import ClientSBERT
from src.federation.client_structure import ClientStructure
from src.federation.strategy import PseudoLabelGenerator

log = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•°ï¼šé˜ˆå€¼è®¡ç®— (Curriculum Learning) ---


def get_dynamic_threshold(start_threshold, end_threshold, total_rounds, current_round):
    """
    è®¡ç®—å½“å‰è½®æ¬¡çš„ä¼ªæ ‡ç­¾é˜ˆå€¼ï¼Œå®ç°è¯¾ç¨‹å­¦ä¹ ç­–ç•¥ (é€æ­¥æé«˜é˜ˆå€¼)ã€‚
    """
    if current_round >= total_rounds:
        return end_threshold

    increment = (end_threshold - start_threshold) / total_rounds
    return start_threshold + current_round * increment


def _fuse_embeddings(struct_dict, sbert_dict, alpha):
    """
    è¾…åŠ©å‡½æ•°ï¼šåŠ æƒèåˆä¸¤ä¸ª Embedding å­—å…¸
    Res = alpha * Struct + (1-alpha) * SBERT
    """
    fused = {}
    # ç¡®ä¿IDå­˜åœ¨ä¸”æœ‰åº
    for eid, s_emb in struct_dict.items():
        if eid in sbert_dict:
            # å½’ä¸€åŒ– (éå¸¸é‡è¦)
            v1 = F.normalize(s_emb, dim=0)
            v2 = F.normalize(sbert_dict[eid], dim=0)
            fused[eid] = alpha * v1 + (1.0 - alpha) * v2
    return fused

# --- è¾…åŠ©å‡½æ•°ï¼šå­—å…¸è½¬æ¢ ---


def to_dict(ids, embs):
    """å°†æœ‰åºçš„idså’Œembeddingsåˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸"""
    return {id: embs[i] for i, id in enumerate(ids)}
# -----------------------------------------------


@hydra.main(config_path="configs", config_name="config", version_base="1.2")
def main(cfg: DictConfig):
    """
    FedAnchor 2.0 ä¸»å…¥å£ (æ”¯æŒ SBERTå¾®è°ƒ å’Œ ç»“æ„è®­ç»ƒ åŒé˜¶æ®µ)
    """
    log.info(f"ğŸš€ Starting Experiment: {cfg.experiment_name}")
    log.info(
        f"âš™ï¸  Task Type: {cfg.task.type} | Mode: {cfg.task.strategy.text_mode}")

    dm = DeviceManager(cfg.system)

    log.info("ğŸ“š Loading Datasets...")
    try:
        task_data = AlignmentTaskData(cfg.data)
    except Exception as e:
        log.exception(f"âŒ Data loading failed: {e}")
        return

    server = Server(cfg)

    # 4. ä»»åŠ¡åˆ†å‘ (Task Dispatch)
    if cfg.task.type == 'sbert':
        log.info("ğŸ”¹ Entering Phase 1: SBERT Fine-tuning")
        c1 = ClientSBERT("C1", cfg, task_data.source, dm)
        c2 = ClientSBERT("C2", cfg, task_data.target, dm)
        run_sbert_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    elif cfg.task.type == 'structure':
        log.info("ğŸ”¹ Entering Phase 2: Structure Training (GCN)")
        log.info("ğŸ—ï¸ Initializing Structure Clients (Loading Frozen SBERT)...")
        c1 = ClientStructure("C1", cfg, task_data.source, dm)
        c2 = ClientStructure("C2", cfg, task_data.target, dm)
        run_structure_workflow(cfg, server, c1, c2, task_data.test_pairs, dm)

    else:
        log.error(f"âŒ Unknown task type: {cfg.task.type}")


def run_sbert_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 1: SBERT æ··åˆå¾®è°ƒå·¥ä½œæµ (å«è¯¾ç¨‹å­¦ä¹ )
    """
    results = []
    rounds = cfg.task.federated.rounds
    base_threshold = cfg.task.strategy.pseudo_threshold
    text_mode = cfg.task.strategy.text_mode

    # é˜ˆå€¼è¯¾ç¨‹å­¦ä¹ å‚æ•°: 0.75 -> 0.85
    threshold_start = 0.75

    for r in range(rounds + 1):
        # åŠ¨æ€è®¡ç®—é˜ˆå€¼ (è¯¾ç¨‹å­¦ä¹ )
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        log.info(
            f"\n{'='*40}\nğŸ”„ [SBERT] Round {r}/{rounds} [{text_mode}] (Thresh: {current_threshold:.4f})\n{'='*40}")

        # 1. Encode
        ids1_desc, emb1_desc = c1.encode('desc')
        ids2_desc, emb2_desc = c2.encode('desc')
        ids1_poly, emb1_poly = c1.encode('polish')
        ids2_poly, emb2_poly = c2.encode('polish')

        # 2. Evaluate
        d1_desc = to_dict(ids1_desc, emb1_desc)
        d2_desc = to_dict(ids2_desc, emb2_desc)
        h_d, m_d = eval_alignment(d1_desc, d2_desc, test_pairs, device='cpu')
        d1_poly = to_dict(ids1_poly, emb1_poly)
        d2_poly = to_dict(ids2_poly, emb2_poly)
        h_p, m_p = eval_alignment(d1_poly, d2_poly, test_pairs, device='cpu')

        log.info(
            f"   ğŸ† Result R{r}: Desc H@1={h_d[1]:.2f}% | Polish H@1={h_p[1]:.2f}%")
        results.append(
            {"round": r, "desc_hits1": h_d[1], "desc_mrr": m_d, "poly_hits1": h_p[1], "poly_mrr": m_p})

        if r == rounds:
            break

        # 3. Strategy (Pseudo Labels)
        log.info(
            f"   Generating Pseudo-labels (Threshold={current_threshold:.4f})...")
        pairs_idx = PseudoLabelGenerator.generate(
            emb1_desc, emb2_desc, current_threshold, device='cpu')
        log.info(f"   ğŸŒ± Found {len(pairs_idx)} high-confidence pairs.")

        if len(pairs_idx) < 50:
            log.warning("   âš ï¸ Too few pairs, skipping training.")
            continue

        # 4. Prepare & Train
        p_idx1 = [p[0] for p in pairs_idx]
        p_idx2 = [p[1] for p in pairs_idx]
        target_desc_c1 = emb2_desc[p_idx2]
        target_desc_c2 = emb1_desc[p_idx1]
        target_poly_c1 = emb2_poly[p_idx2]
        target_poly_c2 = emb1_poly[p_idx1]

        c1.prepare_training_data(p_idx1, target_desc_c1, target_poly_c1)
        c2.prepare_training_data(p_idx2, target_desc_c2, target_poly_for_c2)

        w1, l1 = c1.train()
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train()
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # 5. Aggregate
        server.aggregate([w1, w2])
        c1.model.load_state_dict(server.global_model.state_dict())
        c2.model.load_state_dict(server.global_model.state_dict())
        dm.clean_memory()

    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"{text_mode}_round{rounds}")

    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


def run_structure_workflow(cfg, server, c1, c2, test_pairs, dm):
    """
    Phase 2: ç»“æ„è®­ç»ƒå·¥ä½œæµ (GCN Training) - ç»ˆæä¿®æ­£ç‰ˆ (å«è¯­ä¹‰ä¸€è‡´æ€§è¿‡æ»¤)
    ç­–ç•¥ï¼š
    1. è®­ç»ƒï¼šåŸºäºå½“å‰é”šç‚¹è®­ç»ƒ GCN (åŠ¨æ€è½®æ¬¡ + æ—©åœ)ã€‚
    2. è¯„ä¼°ï¼šä½¿ç”¨ Score Fusion (GCN + SBERT åŠ æƒ)ã€‚
    3. æŒ–æ˜ï¼šä½¿ç”¨ Pure GCN æŒ–æ˜ç»“æ„ç›¸ä¼¼å¯¹ã€‚
    4. è¿‡æ»¤ï¼šä½¿ç”¨ Fixed SBERT è¿›è¡Œè¯­ä¹‰ä¸€è‡´æ€§æ£€æŸ¥ï¼Œå‰”é™¤ç»“æ„å‡é˜³æ€§ã€‚
    5. æ›´æ–°ï¼šå°†é”šç‚¹é”å®šå› Fixed SBERT è¯­ä¹‰ç©ºé—´ã€‚
    """
    results = []
    rounds = cfg.task.federated.rounds
    alpha = cfg.task.eval.alpha
    base_threshold = cfg.task.strategy.pseudo_threshold
    threshold_start = 0.70

    # --- [å…³é”®] 1. é”å®šåŸå§‹è¯­ä¹‰åæ ‡ (Source of Truth) ---
    # åœ¨ä»»ä½•æ›´æ–°å‘ç”Ÿå‰ï¼Œå…‹éš†ä¸€ä»½åŸå§‹çš„ SBERT Anchors
    # è¿™äº›æ˜¯æˆ‘ä»¬çš„"åŒ—ææ˜Ÿ"ï¼Œæ°¸è¿œä¸åº”è¯¥è¢«ä¿®æ”¹
    fixed_sbert_1 = c1.anchor_embeddings.clone().cpu()
    fixed_sbert_2 = c2.anchor_embeddings.clone().cpu()

    # è½¬æ¢ä¸ºå­—å…¸ï¼Œä¾›è¯„ä¼°å‡½æ•°ä½¿ç”¨
    sb_emb1 = to_dict(c1.dataset.ids, fixed_sbert_1)
    sb_emb2 = to_dict(c2.dataset.ids, fixed_sbert_2)

    for r in range(rounds + 1):
        # åŠ¨æ€é˜ˆå€¼ & åŠ¨æ€è½®æ¬¡
        current_threshold = get_dynamic_threshold(
            threshold_start, base_threshold, rounds, r)
        # Round 0 ç»™è¶³ 100 è½®è®© GCN æ”¶æ•›ï¼Œåç»­è½®æ¬¡åªéœ€ 20 è½®å¾®è°ƒ
        current_epochs = cfg.task.federated.local_epochs if r == 0 else 20

        log.info(
            f"\n{'='*40}\nğŸ—ï¸  [Structure] Round {r}/{rounds} (Thresh: {current_threshold:.4f} | Epochs: {current_epochs})\n{'='*40}")

        # ===============================================
        # Step 1: è®­ç»ƒ (Train)
        # ===============================================
        log.info(f"   ğŸš€ Training GCN on current anchors...")
        w1, l1 = c1.train(custom_epochs=current_epochs)
        log.info(f"   ğŸ“‰ C1 Loss: {l1:.6f}")
        w2, l2 = c2.train(custom_epochs=current_epochs)
        log.info(f"   ğŸ“‰ C2 Loss: {l2:.6f}")

        # ===============================================
        # Step 2: èšåˆ (Aggregate)
        # ===============================================
        server.aggregate([w1, w2])
        global_shared = server.get_global_weights()
        c1.model.load_shared_state_dict(global_shared)
        c2.model.load_shared_state_dict(global_shared)
        dm.clean_memory()

        # ===============================================
        # Step 3: è¯„ä¼° (Eval - Score Fusion)
        # ===============================================
        struct_emb1 = c1.get_embeddings()
        struct_emb2 = c2.get_embeddings()

        st_dict1 = to_dict(c1.dataset.ids, struct_emb1)
        st_dict2 = to_dict(c2.dataset.ids, struct_emb2)

        log.info(f"   ğŸ“Š Eval [Score Fusion Alpha={alpha}]...")
        # ä¼ å…¥ fixed SBERT å­—å…¸ï¼Œç¡®ä¿è¯„ä¼°æ ‡å‡†ç»Ÿä¸€
        h_f, m_f = eval_alignment(
            st_dict1, st_dict2, test_pairs,
            sbert1_dict=sb_emb1, sbert2_dict=sb_emb2,
            alpha=alpha, device='cpu'
        )
        log.info(f"   ğŸ† Result R{r}: Hits@1={h_f[1]:.2f}% | MRR={m_f:.4f}")
        results.append({"round": r, "hits1": h_f[1], "mrr": m_f})

        if r == rounds:
            break

        # ===============================================
        # Step 4: æŒ–æ˜ (Mine - Pure GCN)
        # ===============================================
        log.info(
            f"   ğŸ’ Generating Pseudo-labels (GCN Mining, Thresh={current_threshold:.4f})...")

        # ä½¿ç”¨ GCN ç»“æ„å‘é‡å‘ç°æ½œåœ¨å¯¹é½
        # è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ‰¾å‡ºé‚£äº› "SBERT æ²¡çœ‹å‡ºæ¥ï¼Œä½†ç»“æ„ä¸Šå¾ˆåƒ" çš„å®ä½“
        pairs_idx = PseudoLabelGenerator.generate(
            struct_emb1, struct_emb2,
            threshold=current_threshold, device='cpu'
        )

        # ===============================================
        # Step 5: è¯­ä¹‰ä¸€è‡´æ€§è¿‡æ»¤ (Semantic Filter)
        # ===============================================
        filtered_pairs = []
        # è¯­ä¹‰åº•çº¿ï¼šå¦‚æœ SBERT ç›¸ä¼¼åº¦ä½äº 0.3ï¼Œè¯´æ˜è¯­ä¹‰å®Œå…¨ä¸æ²¾è¾¹ï¼Œåˆ¤å®šä¸ºç»“æ„å‡é˜³æ€§
        semantic_filter_thresh = 0.3

        if len(pairs_idx) > 0:
            # æ‰¹é‡æ“ä½œåŠ é€Ÿ
            p1 = torch.tensor([p[0] for p in pairs_idx])
            p2 = torch.tensor([p[1] for p in pairs_idx])

            # å–å‡ºå¯¹åº”çš„ Fixed SBERT å‘é‡
            s1_vecs = F.normalize(fixed_sbert_1[p1], p=2, dim=1)
            s2_vecs = F.normalize(fixed_sbert_2[p2], p=2, dim=1)

            # è®¡ç®—æˆå¯¹ä½™å¼¦ç›¸ä¼¼åº¦
            sem_sims = (s1_vecs * s2_vecs).sum(dim=1)

            # è¿‡æ»¤ï¼šä¿ç•™è¯­ä¹‰ç›¸ä¼¼åº¦ > 0.3 çš„å¯¹å­
            mask = sem_sims > semantic_filter_thresh
            valid_indices = torch.nonzero(mask).squeeze()

            # å¤„ç† Tensor ç»´åº¦è¾¹ç¼˜æƒ…å†µ
            if valid_indices.numel() > 0:
                if valid_indices.ndim == 0:  # åªæœ‰ä¸€ä¸ªå…ƒç´ æ—¶
                    filtered_pairs.append(pairs_idx[valid_indices.item()])
                else:
                    for idx in valid_indices.tolist():
                        filtered_pairs.append(pairs_idx[idx])

            removed_count = len(pairs_idx) - len(filtered_pairs)
            log.info(
                f"   ğŸ” Semantic Filter: Checked {len(pairs_idx)} pairs -> Kept {len(filtered_pairs)} pairs. (Removed {removed_count} noise)")
        else:
            log.warning("   âš ï¸ No structural pairs found to filter.")

        # ===============================================
        # Step 6: æ›´æ–°é”šç‚¹ (Update - Lock to SBERT)
        # ===============================================
        if len(filtered_pairs) > 0:
            p_idx1 = [p[0] for p in filtered_pairs]
            p_idx2 = [p[1] for p in filtered_pairs]

            # C1 çš„æ–°ç›®æ ‡ï¼šæ—¢ç„¶ p_idx1 å¯¹åº” p_idx2ï¼Œé‚£å°±è®© p_idx1 å»å­¦ p_idx2 çš„ SBERT å‘é‡
            # è¿™æ ·ä¿è¯äº†ç›®æ ‡æ°¸è¿œåœ¨è¯­ä¹‰ç©ºé—´å†…ï¼Œä¸ä¼šå‘ç”Ÿ GCN äº’å·å¯¼è‡´çš„æ¼‚ç§»
            new_anchors_for_c1 = fixed_sbert_2[p_idx2]
            new_anchors_for_c2 = fixed_sbert_1[p_idx1]

            c1.update_anchors(p_idx1, new_anchors_for_c1)
            c2.update_anchors(p_idx2, new_anchors_for_c2)

            log.info(
                f"   âœ… Anchors Expanded: +{len(filtered_pairs)} pairs (Targets locked to Fixed SBERT).")
        else:
            log.warning("   âš ï¸ No new anchors added this round.")

    # Save
    if cfg.task.checkpoint.save_best:
        server.save_model(suffix=f"structure_round{rounds}")
    log_experiment_result(cfg.experiment_name,
                          cfg.data.name, results[-1], config=cfg)


if __name__ == "__main__":
    os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"
    main()
