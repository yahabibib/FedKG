# ğŸ“„ bert_finetuner.py
# ã€MLMç‰ˆã€‘æ‰§è¡Œ Masked Language Modeling ä»»åŠ¡

import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer
import config
import logging
import os
import shutil

# è‡ªå®šä¹‰ Dataset


class TripleDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len=32):
        # ä¸‰å…ƒç»„å¥å­å¾ˆçŸ­ï¼Œ32 è¶³å¤Ÿäº†ï¼Œçœæ˜¾å­˜å¿«
        self.encodings = tokenizer(
            texts, return_tensors='pt', max_length=max_len, truncation=True, padding='max_length')

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

    def __len__(self):
        return len(self.encodings.input_ids)


def fine_tune_with_mlm(model_path, sentences, save_path, epochs=3, batch_size=32):
    """
    å¯¹ BERT è¿›è¡Œ MLM é¢„è®­ç»ƒ (Domain Adaptive Pre-training)
    """
    logging.info(
        f"   ğŸ”§ [MLM Pre-training] Starting with {len(sentences)} sentences...")

    # 1. åŠ è½½ HuggingFace åŸç”Ÿæ¨¡å‹ (æ”¯æŒ MaskedLM)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForMaskedLM.from_pretrained(model_path)

    # å¼ºåˆ¶ç§»åŠ¨åˆ° MPS/CUDA
    if config.DEVICE.type == 'mps':
        model.to("mps")
    elif config.DEVICE.type == 'cuda':
        model.to("cuda")

    # 2. æ•°æ®é›†
    dataset = TripleDataset(sentences, tokenizer)

    # 3. è‡ªåŠ¨ Mask (15% æ¦‚ç‡)
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=True, mlm_probability=0.15)

    # 4. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=save_path,
        overwrite_output_dir=True,
        num_train_epochs=epochs,
        per_device_train_batch_size=batch_size,
        save_steps=5000,
        save_total_limit=1,
        logging_steps=50,  # é¢‘ç¹æ‰“å°æ—¥å¿—
        learning_rate=2e-5,
        weight_decay=0.01,
        report_to="none",
        use_mps_device=True if config.DEVICE.type == 'mps' else False,
        dataloader_pin_memory=False  # ä¼˜åŒ– MPS å†…å­˜
    )

    # 5. Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=dataset,
    )

    logging.info("   ğŸš€ Starting MLM training...")
    trainer.train()

    # 6. ä¿å­˜ (å­˜ä¸º HuggingFace æ ¼å¼ï¼ŒSBERT ä¹Ÿèƒ½è¯»)
    if os.path.exists(save_path):
        shutil.rmtree(save_path)
    trainer.save_model(save_path)
    tokenizer.save_pretrained(save_path)

    logging.info(f"   âœ… Structure-Aware BERT saved to: {save_path}")

    # æ¸…ç†
    del model, trainer
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()

    return save_path
